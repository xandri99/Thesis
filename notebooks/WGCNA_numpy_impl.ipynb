{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "# import cupy as cp\n",
    "\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import scipy.cluster.hierarchy as scipy_hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL CONFIGURATION FOR THE ANALYSIS:\n",
    "# Adjacency type affects how the adjacency matrix is calculated. The signed method, im not sure if it makes sense\n",
    "adjacency_type = \"unsigned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS FOR PLOTTING FIGURES\n",
    "PLOTS_WANTED = False\n",
    "dpi_general = 500\n",
    "\n",
    "# Settings for printing dataframes\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the terminal outputs\n",
    "ENDC = \"\\033[0m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "UNDERLINE = \"\\033[4m\"\n",
    "\n",
    "OKBLUE = \"\\033[94m\"\n",
    "OKGREEN = \"\\033[92m\"\n",
    "WARNING = \"\\033[93m\"\n",
    "FAIL = \"\\033[91m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Move out of the notebook folder to access datasets\n",
    "working_dir = os.getcwd()\n",
    "working_dir = working_dir.strip('notebooks')\n",
    "\n",
    "\n",
    "\n",
    "## Load the dataset\n",
    "# Transcriptomics Data - We assume a dataset structure were col are the genes, and rows are the samples\n",
    "transcriptomics_dataset_testing_dir = working_dir + 'data/5xFAD_paper/expressionList.csv'  \n",
    "transcriptomics_dataset_testing = pd.read_csv(transcriptomics_dataset_testing_dir, index_col=0)\n",
    "\n",
    "# Sample info and Clinical Traits Data\n",
    "sample_info_traits_dir = working_dir + 'data/5xFAD_paper/sampleInfo.csv'\n",
    "trait_dataset = pd.read_csv(sample_info_traits_dir)\n",
    "\n",
    "# Figures Saving output dir\n",
    "figures_dir = working_dir + 'results/WGCNA_figures_5xFAD/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "    print(f\"{BOLD}{OKBLUE}Creating directory to save results and figures...{ENDC}\")\n",
    "\n",
    "\n",
    "#transcriptomics_dataset = transcriptomics_dataset_testing\n",
    "\n",
    "\n",
    "## Make a subset to save RAM\n",
    "subset_dataset_size = 30000\n",
    "transcriptomics_dataset = transcriptomics_dataset_testing.iloc[:, :subset_dataset_size] \n",
    "\n",
    "# RAM usage estimation in GB\n",
    "RAM_estimate = (subset_dataset_size * subset_dataset_size * 8) / (1024**3)\n",
    "print(f\"The aproximated RAM to analyse this size of dataset is: {RAM_estimate} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL DATA FROM THE LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING REAL UNPUBLISHED DATA    -     NO PUSHING FOR THE RESULTS\n",
    "\n",
    "# Move out of the notebook folder to access datasets\n",
    "working_dir = os.getcwd()\n",
    "working_dir = working_dir.strip('notebooks')\n",
    "data_dir = working_dir + 'data/PROTECTED_DATA/BGI_Expression_Data/'\n",
    "\n",
    "\n",
    "\n",
    "## Load the dataset\n",
    "# Transcriptomics Data - We assume a dataset structure were col are the genes, and rows are the samples\n",
    "# transcriptomics_dataset_dir = data_dir + 'CRC.SW.mRNA.symbol.count.csv'  \n",
    "# transcriptomics_dataset = pd.read_csv(transcriptomics_dataset_dir, index_col=0)\n",
    "\n",
    "# Transcriptomics Data - We assume a dataset structure were col are the genes, and rows are the samples\n",
    "transcriptomics_TPM_dataset_dir = data_dir + 'CRC.SW.mRNA.symbol.TPM.csv'  \n",
    "transcriptomics_dataset = pd.read_csv(transcriptomics_TPM_dataset_dir, index_col=0)\n",
    "\n",
    "# Sample info and Clinical Traits Data\n",
    "# Sample info and Clinical Traits Data\n",
    "sample_info_traits_dir = data_dir + 'Sample_Info_Selection.csv'\n",
    "trait_dataset = pd.read_csv(sample_info_traits_dir)\n",
    "trait_dataset\n",
    "\n",
    "\n",
    "# Figures Saving output dir\n",
    "figures_dir = working_dir + 'results/WGCNA_figures_BGI_TPM/'\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "    print(f\"{BOLD}{OKBLUE}Creating directory to save results and figures...{ENDC}\")\n",
    "\n",
    "\n",
    "'''\n",
    "## Make a subset to save RAM\n",
    "subset_dataset_size = 2000\n",
    "transcriptomics_dataset = transcriptomics_dataset.iloc[:, :subset_dataset_size] \n",
    "\n",
    "# RAM usage estimation in GB\n",
    "RAM_estimate = (subset_dataset_size * subset_dataset_size * 8) / (1024**3)\n",
    "print(f\"The aproximated RAM to analyse this size of dataset is: {RAM_estimate} GB\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Data Preprocessing (Normalization)\n",
    "## Preprocessing: removing obvious outlier on genes and samples\n",
    "print(f\"{BOLD}{OKBLUE}Pre-processing...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "## Prepare and clean data\n",
    "# Remove genes expressed under this cutoff number along samples - use clustering of samples\n",
    "\n",
    "\n",
    "\n",
    "## Remove genes with no variation across samples (0 vectors) \n",
    "transcriptomics_dataset_filtered = transcriptomics_dataset.loc[:, (transcriptomics_dataset != 0).any(axis=0)] # Actually droping columns with 0 variation\n",
    "\n",
    "# Also print how many genes have been removed in this steo\n",
    "num_genes_removed = transcriptomics_dataset.shape[1] - transcriptomics_dataset_filtered.shape[1]\n",
    "print(f\"{BOLD}{WARNING}{num_genes_removed} genes were removed due to having 0 variation across samples...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "# NOTES: Maybe no onlyt genes with no variation, but small variation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Done...{ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Constructing a Co-expression Similarity Matrix (Correlation Matrix)\n",
    "'''\n",
    "The Correlation matrix, also known as Co-expression Matrix or Similarity matrix is calculated by\n",
    "calculating the correlation between the expression profiles of all genes. The expression profile of \n",
    "a gene is a vector containing the expression levels of that gene for all samples (pacients)\n",
    "'''\n",
    "\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Calculating Correlation Matrix...{ENDC}\")\n",
    "\n",
    "## Calculate the correlation matrix using Pearson correlation\n",
    "# Since Pandas correlation is extreamly slow, we switched to numpy\n",
    "# for CuPy implementaiton, see WGCNA_CuPy_impl.ipynb\n",
    "np_transcriptomics_dataset = transcriptomics_dataset_filtered.T.to_numpy()\n",
    "correlation_matrix_np = np.corrcoef(np_transcriptomics_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Enforce ranges in matrix python handling Floating poin incorrectly \n",
    "correlation_matrix_np = np.clip(correlation_matrix_np, -1.0, 1.0)\n",
    "print(f\"{BOLD}{FAIL}Enforced max and min values to compensate for Floating point misshandles in python.\\n\\n{ENDC}\")\n",
    "\n",
    "# Also fix the diagonal\n",
    "np.fill_diagonal(correlation_matrix_np, 1)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Done.\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "## Plotting the heatmap\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Correlation Matrix...{ENDC}\")\n",
    "    title_figure = 'Gene Expression Correlation Matrix Heatmap'\n",
    "    \n",
    "    # using pcolorfast for speed\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    cax = ax.pcolorfast(correlation_matrix_np, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Genes', fontsize=10)\n",
    "    plt.ylabel('Genes', fontsize=10)\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Done{ENDC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST TO ENSURE THAT PYTHON IS NOT BREAKING THE MATHS BY POOR-HANDELING THE FLOATS\n",
    "\n",
    "# Check if any values are out of range\n",
    "min_value_allowed = -1\n",
    "max_value_allowed = 1\n",
    "\n",
    "smaller_values_check = not((correlation_matrix_np < min_value_allowed).any().any())\n",
    "bigger_values_check = not((correlation_matrix_np > max_value_allowed).any().any())\n",
    "\n",
    "# Print check of values out of range\n",
    "print(f'Are all values >= {min_value_allowed}?  {smaller_values_check}') \n",
    "print(f'Are all values <= {max_value_allowed}?  {bigger_values_check}')\n",
    "print('\\n')\n",
    "\n",
    "# For visual inspection print biggest and smallest value in the Dataframe\n",
    "print(f'The biggest value in this matrix is: {correlation_matrix_np.max().max()}\\\n",
    "\\nThe smallest value in this matrix is: {correlation_matrix_np.min().min()}')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Check if it is an issue with the diagonal\n",
    "expected_diagonal = 1\n",
    "diagonal = np.diag(correlation_matrix_np)\n",
    "all_equal_to_expected = np.all(diagonal == expected_diagonal)\n",
    "print(f'All the values in the diagonal are euqal to the expected value of {expected_diagonal}?   {all_equal_to_expected}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Transforming into an adjacency matrix using a soft threshold power\n",
    "print(f\"{BOLD}{OKBLUE}Transforming Correlation Matrix into Adjacency Matrix...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "####################################### FUNCTIONS #######################################\n",
    "\n",
    "### Function that checks if the adjacency matrix resembles a scale-free network topology (following a power-law distribution).\n",
    "def scaleFreeFitIndex(connectivity, block_size=10):  # values and code highly taken from pyWGCNA, only small adaptations\n",
    "    \"\"\"\n",
    "    Calculates several fitting statistics to evaluate scale free topology fit.\n",
    "\n",
    "\n",
    "    block_size = Size of the blocks in which the connectivity is discretized\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Ensure all data is non zero\n",
    "    connectivity = connectivity[connectivity > 0]\n",
    "\n",
    "    # Turn the connectivity into a dataframe, and discretize into blocks of block_size size\n",
    "    connectivity = pd.DataFrame({'data': connectivity})\n",
    "    connectivity['discretized_connectivity'] = pd.cut(connectivity['data'], block_size)\n",
    "\n",
    "    # Calculate mean connectivity, probability density for each block\n",
    "    per_block_stats = connectivity.groupby('discretized_connectivity', observed=False)['data'].agg(['mean', 'count']) \\\n",
    "                            .reset_index().rename(columns={'mean': 'mean_connectivity_per_block', 'count': 'count_per_block'})\n",
    "    per_block_stats['probability_density_per_block'] = per_block_stats['count_per_block'] / len(connectivity)\n",
    "\n",
    "    # Handle blocks with no values or zero connectivity\n",
    "    breaks = np.linspace(start=connectivity['data'].min(), stop=connectivity['data'].max(), num=block_size + 1)\n",
    "    mid_points_blocks = 0.5 * (breaks[:-1] + breaks[1:])  # Mid-points of blocks\n",
    "\n",
    "    # Impute missing or zero values with block midpoints\n",
    "    for i, row in per_block_stats.iterrows():\n",
    "        if pd.isnull(row['mean_connectivity_per_block']) or row['mean_connectivity_per_block'] == 0:\n",
    "            per_block_stats.at[i, 'mean_connectivity_per_block'] = mid_points_blocks[i]\n",
    "\n",
    "\n",
    "    # Logarithmic transformation\n",
    "    per_block_stats['log_mean_conn'] = np.log10(per_block_stats['mean_connectivity_per_block'])\n",
    "    per_block_stats['log_prob_distr_conn'] = np.log10(per_block_stats['probability_density_per_block'] + 1e-9)\n",
    "\n",
    "    # Linear regression Model\n",
    "    simple_linear_regression_model = ols('log_prob_distr_conn ~ log_mean_conn', data=per_block_stats).fit()\n",
    "    rsquared = simple_linear_regression_model.rsquared\n",
    "    slope = simple_linear_regression_model.params['log_mean_conn']\n",
    "    \n",
    "    # Quadratic Regression Model for Adjusted R-squared\n",
    "    quadratic_regression_model = ols('log_prob_distr_conn ~ log_mean_conn + I(log_mean_conn**2)', data=per_block_stats).fit()\n",
    "    rsquared_adj = quadratic_regression_model.rsquared_adj\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Rsquared.SFT': [rsquared],\n",
    "        'slope.SFT': [slope],\n",
    "        'Rsquared Adjusted': [rsquared_adj]\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "### Function that analyses the network topology of the dataset\n",
    "def pickSoftThreshold(correlation_matrix, RsquaredCut, MeanCut):\n",
    "    \"\"\"\n",
    "    Analyzes scale-free topology for multiple soft thresholding powers.\n",
    "    Soft power-thresholding is a value used to power each value of the correlation matrix of the genes to that threshold.\n",
    "    The assumption is that by raising the correlation values to a power, we will reduce the noise of the correlations in\n",
    "    the adjacency matrix, therefore putting in relevance important links and tuning down the noise.\n",
    "\n",
    "    To pick up the threshold, the pickSoftThreshold function calculates for each possible power if the network resembles\n",
    "    a scale-free network topology (following a power-law distribution).\n",
    "    The power which produce a higher similarity with a scale-free network is the one returned.\n",
    "\n",
    "    This is critical in WGCNA, as the premise of the method is that biological networks often exhibit scale-free properties, \n",
    "    meaning that a few nodes (genes) are highly connected, while most have few connections.\n",
    "\n",
    "    \n",
    "    RsquaredCut: Threshold for the R^2 statistic\n",
    "    MeanCut: Threshold for the Mean Connectivity statistic\n",
    "    \"\"\"\n",
    "\n",
    "    powerVector = list(range(1, 11)) + list(range(12, 21, 2))  # values taken from pyWGCNA package\n",
    "\n",
    "    # Initialize output DataFrame\n",
    "    results = pd.DataFrame(index=powerVector, columns=[\"Power\"])\n",
    "\n",
    "\n",
    "\n",
    "    ## Build and adjacency matrix with each power, and test its fit to scale-free network topology\n",
    "    for power in powerVector:\n",
    "        # Calculate adjacency matrix from the correlation_matrix with each power\n",
    "        adjacency_matrix = np.power(np.abs(correlation_matrix), power)\n",
    "        \n",
    "        # Calculate connectivity for each node/gene\n",
    "        connectivity = adjacency_matrix.sum(axis=0) - 1  # we remove the autocorrelation for each row\n",
    "        \n",
    "        # Check the resemblance to a scale-free network topology \n",
    "        fit_values = scaleFreeFitIndex(connectivity)\n",
    "        \n",
    "        # Store results\n",
    "        results.loc[power, \"Power\"] = power\n",
    "        results.loc[power, \"R²\"] = fit_values['Rsquared.SFT'].values[0]\n",
    "        results.loc[power, \"Slope\"] = fit_values['slope.SFT'].values[0]\n",
    "        results.loc[power, \"Exponential R² Adjusted\"] = fit_values['Rsquared Adjusted'].values[0]\n",
    "        results.loc[power, \"mean(connectivity)\"] = connectivity.mean()\n",
    "        results.loc[power, \"median(connectivity)\"] = connectivity.median()\n",
    "        results.loc[power, \"max(connectivity)\"] = connectivity.max()\n",
    "    print(results)\n",
    "\n",
    "\n",
    "    ## Select the power with the best fit\n",
    "    valid_powers = results[(results[\"R²\"] > RsquaredCut) & (results[\"mean(connectivity)\"] < MeanCut)]\n",
    "    if not valid_powers.empty:\n",
    "        optimal_power = valid_powers.index[0]\n",
    "    else:\n",
    "        optimal_power = results[\"R²\"].idxmax()\n",
    "\n",
    "\n",
    "    print(f\"{BOLD}{OKGREEN}The optimal Power-Threshold found is {optimal_power}.{ENDC}\")\n",
    "    return optimal_power, results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Choosing the soft-thresholding power: analysis of network topology based on the dataset\n",
    "RsquaredCut = 0.75   # Values from pyWGCNA\n",
    "MeanCut = 100       # Values from pyWGCNA\n",
    "\n",
    "# Temporary turning back to pandas, but should be reimplemented with numpy\n",
    "correlation_matrix = pd.DataFrame(correlation_matrix_np, columns=transcriptomics_dataset_filtered.columns, index=transcriptomics_dataset_filtered.columns)\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Starting Soft Power-Thresholding algorithmic search ...{ENDC}\")\n",
    "optimal_power, results = pickSoftThreshold(correlation_matrix, RsquaredCut, MeanCut) \n",
    "print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "# Plotting the results\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving Scale-Free Topology fit analysis...{ENDC}\")\n",
    "    title_figure1 = 'Scale-Free Topology Analysis'\n",
    "    title_figure2 = 'Mean Connectivity'\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plotting Scale-Free Topology Analysis\n",
    "    axs[0].plot(results.index, results[\"R²\"], marker='o', linestyle='-', label='Scale-Free Topology Fit R^2')\n",
    "    axs[0].axhline(y=RsquaredCut, color='red', linestyle='--', label=f'R^2 cut-off: {RsquaredCut}')\n",
    "    axs[0].axvline(x=optimal_power, color='green', linestyle='-', label=f'Optimal Power: {optimal_power}')\n",
    "    axs[0].set_xlabel('Soft Thresholding Power')\n",
    "    axs[0].set_ylabel('Scale-Free Topology Fit R^2')\n",
    "    axs[0].set_title(title_figure1, fontsize=20)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plotting Mean Connectivity vs. Soft Thresholding Power\n",
    "    mean_connect_optimal_power = results.loc[optimal_power, 'mean(connectivity)']\n",
    "    axs[1].plot(results.index, results['mean(connectivity)'], marker='o', linestyle='-', color='blue', label='Mean Connectivity')\n",
    "    axs[1].axhline(y=mean_connect_optimal_power, color='red', linestyle='--', label=f'Mean Conn at Opt-Power: {mean_connect_optimal_power:.3f}')\n",
    "    axs[1].axvline(x=optimal_power, color='green', linestyle='--', label=f'Optimal Power: {optimal_power}')\n",
    "    axs[1].set_xlabel('Soft Thresholding Power')\n",
    "    axs[1].set_ylabel('Mean Connectivity')\n",
    "    axs[1].set_title(title_figure2, fontsize=20)\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir + title_figure1 + \" and \" + title_figure2, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Making the adjacency  matrix out of the correlation matrix, based on the soft-thresholding power selected\n",
    "print(f\"{BOLD}{OKBLUE}Calculating Adjacency Matrix...{ENDC}\")\n",
    "\n",
    "adjacency_type = adjacency_type\n",
    "if adjacency_type == \"unsigned\":\n",
    "    adjacency_matrix_np = np.power(np.abs(correlation_matrix_np), optimal_power)\n",
    "elif adjacency_type == 'signed':\n",
    "    correlation_matrix_np = (correlation_matrix_np + 1) / 2\n",
    "    adjacency_matrix_np = np.power(correlation_matrix_np, optimal_power)\n",
    "\n",
    "# Enforce ranges in matrix\n",
    "adjacency_matrix_np = np.clip(adjacency_matrix_np, 0.0, 1.0)\n",
    "\n",
    "print(f\"{BOLD}{FAIL}Enforced max and min values to compensate for Floating point misshandles in python.\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "# Also fix the diagonal\n",
    "np.fill_diagonal(adjacency_matrix_np, 1)\n",
    "    \n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Plotting the heatmap\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Adjacency Matrix...{ENDC}\")\n",
    "    title_figure = 'Adjacency Matrix Heatmap'\n",
    "\n",
    "    # plt.figure(figsize=(15, 15))\n",
    "    # sns.heatmap(adjacency_matrix, cmap='coolwarm', xticklabels=False, yticklabels=False)\n",
    "\n",
    "    # using pcolorfast for speed\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    cax = ax.pcolorfast(adjacency_matrix_np, cmap='coolwarm', vmin=0, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Genes', fontsize=10)\n",
    "    plt.ylabel('Genes', fontsize=10)\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST TO ENSURE THAT PYTHON IS NOT BREAKING THE MATHS BY POOR-HANDELING THE FLOATS\n",
    "\n",
    "# Check if any values are out of range\n",
    "min_value_allowed = 0\n",
    "max_value_allowed = 1\n",
    "\n",
    "smaller_values_check = not((adjacency_matrix_np < min_value_allowed).any().any())\n",
    "bigger_values_check = not((adjacency_matrix_np > max_value_allowed).any().any())\n",
    "\n",
    "# Print check of values out of range\n",
    "print(f'Are all values >= {min_value_allowed}?  {smaller_values_check}') \n",
    "print(f'Are all values <= {max_value_allowed}?  {bigger_values_check}')\n",
    "print('\\n')\n",
    "\n",
    "# For visual inspection print biggest and smallest value in the Dataframe\n",
    "print(f'The biggest value in this matrix is: {adjacency_matrix_np.max().max()}\\\n",
    "\\nThe smallest value in this matrix is: {adjacency_matrix_np.min().min()}')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Check if it is an issue with the diagonal\n",
    "expected_diagonal = 1\n",
    "diagonal = np.diag(adjacency_matrix_np)\n",
    "all_equal_to_expected = np.all(diagonal == expected_diagonal)\n",
    "print(f'All the values in the diagonal are euqal to the expected value of {expected_diagonal}?   {all_equal_to_expected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Converting adjacency matrix into a topological overlap matrix (TOM)\n",
    "print(f\"{BOLD}{OKBLUE}Transforming Adjacency Matrix into Topological Overlap Matrix (TOM)...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "####################################### FUNCTIONS #######################################\n",
    "def calculate_tom(numeric_adjacency_matrix, TOMDenom, adjacency_type):\n",
    "    '''\n",
    "    Revisit this function now that i have the source code of the R package in C\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    This function takes the adjacency matrix and calculates the Topological Overlap Matrix (TOM) from it.\n",
    "    This matrix is a similarity matrix, using as the similarity metric the amount of genes that two genes \n",
    "    have in common, therefore the overlap of neighbors between two genes.\n",
    "\n",
    "    \n",
    "    TOMType:  Unsigned or Signed topologies\n",
    "    TOMDenom: A character string specifying the TOM variant to be used. Recognized values are \"min\" giving the standard TOM described \n",
    "                in Zhang and Horvath (2005), and \"mean\" in which the min function in the denominator is replaced by mean. The \"mean\" \n",
    "                may produce better results but at this time should be considered experimental.\n",
    "    '''\n",
    "\n",
    "    # Turn the Adjacency matrix diagonl to 0 for calculations\n",
    "    np.fill_diagonal(numeric_adjacency_matrix, 0)\n",
    "\n",
    "    # Calculate numerator as A^2\n",
    "    numerator = np.dot(numeric_adjacency_matrix, numeric_adjacency_matrix)\n",
    "\n",
    "    # Denominator calculation based on TOMDenom selection\n",
    "    row_sum = numeric_adjacency_matrix.sum(axis=1)\n",
    "    col_sum = numeric_adjacency_matrix.sum(axis=0)\n",
    "    # Calculations deppending on the selectod method\n",
    "    if TOMDenom == 'min':\n",
    "        denominator = np.minimum.outer(row_sum, col_sum)\n",
    "    elif TOMDenom == 'mean':\n",
    "        denominator = (np.outer(row_sum, np.ones_like(row_sum)) + np.outer(np.ones_like(col_sum), col_sum)) / 2\n",
    "\n",
    "\n",
    "     # Numerator adjustment for unsigned and signed topologies\n",
    "    if adjacency_type == 'unsigned':\n",
    "        tom = (numerator + numeric_adjacency_matrix) / (denominator + 1 - numeric_adjacency_matrix)\n",
    "    elif adjacency_type == 'signed':\n",
    "        tom = np.abs(numerator + numeric_adjacency_matrix) / (denominator + 1 - np.abs(numeric_adjacency_matrix))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Set diagonal to 1 as per TOM definition\n",
    "    np.fill_diagonal(tom, 1)  \n",
    "\n",
    "    # Handle NaN values and set them to zero\n",
    "    tom = np.nan_to_num(tom, nan=0)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    #tom = pd.DataFrame(tom, index=adjacency_matrix.index, columns=adjacency_matrix.columns)\n",
    "\n",
    "    return tom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Calculating the TOM from the Adjacency Matrix\n",
    "# TOMDenom must be either 'min' or 'mean'. More explanation in the function itself\n",
    "TOMDenom = \"min\"\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Calculating the TOM...{ENDC}\")\n",
    "simTOM_np = calculate_tom(adjacency_matrix_np, TOMDenom, adjacency_type)\n",
    "dissTOM_np = 1 - simTOM_np\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Plotting the heatmap\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the TOM...{ENDC}\")\n",
    "    title_figure = 'Topological Overlap Matrix (TOM) Heatmap'\n",
    "\n",
    "    # plt.figure(figsize=(15, 15))\n",
    "    # sns.heatmap(simTOM, cmap='coolwarm', xticklabels=False, yticklabels=False)\n",
    "\n",
    "    # using pcolorfast for speed\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    cax = ax.pcolorfast(simTOM_np, cmap='coolwarm', vmin=0, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Genes', fontsize=10)\n",
    "    plt.ylabel('Genes', fontsize=10)\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST TO ENSURE THAT PYTHON IS NOT BREAKING THE MATHS BY POOR-HANDELING THE FLOATS\n",
    "\n",
    "# Check if any values are out of range\n",
    "min_value_allowed = 0\n",
    "max_value_allowed = 1\n",
    "\n",
    "smaller_values_check = not((simTOM_np < min_value_allowed).any().any())\n",
    "bigger_values_check = not((simTOM_np > max_value_allowed).any().any())\n",
    "\n",
    "# Print check of values out of range\n",
    "print(f'Are all values >= {min_value_allowed}?  {smaller_values_check}') \n",
    "print(f'Are all values <= {max_value_allowed}?  {bigger_values_check}')\n",
    "print('\\n')\n",
    "\n",
    "# For visual inspection print biggest and smallest value in the Dataframe\n",
    "print(f'The biggest value in this matrix is: {simTOM_np.max().max()}\\\n",
    "\\nThe smallest value in this matrix is: {simTOM_np.min().min()}')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Check if it is an issue with the diagonal\n",
    "expected_diagonal = 1\n",
    "diagonal = np.diag(simTOM_np)\n",
    "all_equal_to_expected = np.all(diagonal == expected_diagonal)\n",
    "print(f'All the values in the diagonal are euqal to the expected value of {expected_diagonal}?   {all_equal_to_expected}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Hierarchical clustering\n",
    "## We use a dendogram where the distances are taken from the dissTOM \n",
    "\n",
    "\n",
    "print(f\"{BOLD}{OKBLUE}Doing Hierarchical clustering over the dissimilarity TOM (1-TOM)...{ENDC}\")\n",
    "# Take only one half of the TOM matrix, since it is square and symmetrix\n",
    "condensed_dissTOM = squareform(dissTOM_np, checks=False)\n",
    "\n",
    "## Do the Hierarchical clustering\n",
    "# The different methods can be: \"single\", \"complete\", \"average\", \"weighted\", \"centroid\"]\n",
    "method = \"average\"  \n",
    "linkage_matrix = scipy_hierarchy.linkage(condensed_dissTOM, method=method)\n",
    "print(f\"{BOLD}{OKBLUE}Done...\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "## Plot the dendrogram\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Dendogram from the Hierarchical clustering...{ENDC}\")\n",
    "    title_figure = 'Dendogram from the Hierarchical clustering'\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    scipy_hierarchy.dendrogram(linkage_matrix, truncate_mode=None, color_threshold=0)\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Genes', fontsize=10)\n",
    "    plt.ylabel('Distance taken from the TOM', fontsize=10)\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 6: Module identification\n",
    "print(f\"{BOLD}{OKBLUE}Transforming Adjacency Matrix into Topological Overlap Matrix (TOM)...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "####################################### FUNCTIONS #######################################\n",
    "def identify_modules(linkage_matrix, dist_matrix, min_memb_cluster, deep_split):\n",
    "    \"\"\"\n",
    "    Enhanced module identification mimicking cutreeHybrid's functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_d = max(linkage_matrix[:, 2])\n",
    "    cutHeight = np.percentile(linkage_matrix[:, 2], 95)\n",
    "\n",
    "\n",
    "\n",
    "    ## Cluster Assignment\n",
    "    cluster_labels = scipy_hierarchy.fcluster(linkage_matrix, t=cutHeight, criterion='distance')\n",
    "\n",
    "\n",
    "\n",
    "    ## Core Scatter and Gap Analysis\n",
    "    # calculate the average within-cluster distance (as a proxy for core scatter) \n",
    "    # and the minimum inter-cluster distance (as a proxy for the gap)\n",
    "    n_clusters = np.max(cluster_labels)\n",
    "    core_scatters = []\n",
    "    gaps = []\n",
    "    \n",
    "    for i in range(1, n_clusters + 1):\n",
    "        within_cluster_distances = dist_matrix[np.ix_(cluster_labels == i, cluster_labels == i)]\n",
    "        between_cluster_distances = dist_matrix[np.ix_(cluster_labels == i, cluster_labels != i)]\n",
    "        \n",
    "        # Calculate core scatter as the average within-cluster distance\n",
    "        core_scatter = np.mean(within_cluster_distances[np.triu_indices_from(within_cluster_distances, k=1)])\n",
    "        core_scatters.append(core_scatter)\n",
    "        \n",
    "        # Calculate gap as the minimum between-cluster distance\n",
    "        gap = np.min(between_cluster_distances) if between_cluster_distances.size > 0 else np.inf\n",
    "        gaps.append(gap)\n",
    "    \n",
    "    # DataFrame with these metrics for each cluster.\n",
    "    cluster_metrics = pd.DataFrame({'Cluster': range(1, n_clusters + 1), 'CoreScatter': core_scatters, 'Gap': gaps})\n",
    "\n",
    "\n",
    "\n",
    "    ## Handling Small Clusters and Singletons\n",
    "    # merging small clusters and reassigning singletons.\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cluster_sizes = {cluster: np.sum(cluster_labels == cluster) for cluster in unique_clusters}\n",
    "    small_clusters = [cluster for cluster, size in cluster_sizes.items() if size < min_memb_cluster]\n",
    "    \n",
    "    # Process small clusters and singletons\n",
    "    for small_cluster in small_clusters:\n",
    "        members = np.where(cluster_labels == small_cluster)[0]\n",
    "        \n",
    "        # If it's a singleton, find the closest cluster to merge into\n",
    "        if len(members) == 1:\n",
    "            distances = dist_matrix[members, :]\n",
    "            # Mask distances within the same cluster\n",
    "            distances[distances == 0] = np.inf\n",
    "            closest_cluster = np.argmin(np.min(distances, axis=1))\n",
    "            new_label = cluster_labels[closest_cluster]\n",
    "            \n",
    "            # Reassign the singleton to the closest cluster\n",
    "            cluster_labels[members] = new_label\n",
    "        else:\n",
    "            # For small clusters, consider merging logic or leave as is for simplicity\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    return module_assignment, cut_height\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def identify_modules_simple_version(linkage_matrix, min_memb_cluster):\n",
    "    '''\n",
    "    Very Basic first approach (written out of what I know about tree cutting) to extract flat clusters (gene modules)\n",
    "    out of the Dendogram, trying to make it balanced with dynamic parameters.\n",
    "\n",
    "    Genes that are not assigned to any module will be identified as Module 0\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Dynamically set parameters based on dendrogram size\n",
    "    min_memb_cluster = min_memb_cluster\n",
    "    height_percentile = 25  # Use the percentile of heights to set sensitivity\n",
    "    cut_height_percentile = np.percentile(linkage_matrix[:, 2], height_percentile)\n",
    "    \n",
    "    # Set cut_height based on dynamic sensitivity\n",
    "    max_dendro_height = np.max(linkage_matrix[:, 2])\n",
    "    sensitivity = max_dendro_height / cut_height_percentile\n",
    "    cut_height = max_dendro_height / sensitivity\n",
    "\n",
    "    # Form flat clusters from the dendogram\n",
    "    cluster_labels = scipy_hierarchy.fcluster(linkage_matrix, t=cut_height, criterion='distance')\n",
    "\n",
    "    # Map genes to their cluster labels in a dataframe\n",
    "    module_assignment = pd.DataFrame({'Gene': range(1, len(cluster_labels) + 1), 'Module': cluster_labels})\n",
    "\n",
    "    # Filter out small modules and join them to the Module 0 (no cluster)\n",
    "    module_sizes = module_assignment['Module'].value_counts()\n",
    "    small_modules = module_sizes[module_sizes < min_memb_cluster].index\n",
    "    module_assignment['Module'] = module_assignment['Module'].apply(lambda x: 0 if x in small_modules else x)\n",
    "\n",
    "    # Reassign module labels to be consecutive for non-zero modules\n",
    "    non_zero_modules = module_assignment[module_assignment['Module'] != 0]['Module']\n",
    "    unique_non_zero_modules = pd.Categorical(non_zero_modules).codes + 1\n",
    "    module_assignment.loc[module_assignment['Module'] != 0, 'Module'] = unique_non_zero_modules\n",
    "\n",
    "    return module_assignment, cut_height\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_memb_cluster = 20\n",
    "deep_split=2 # Provides a rough control over sensitivity to cluster splitting. The higher the value, the more and smaller clusters will be produced. (default = 1)\n",
    "module_assignment, cut_height = identify_modules_simple_version(linkage_matrix, min_memb_cluster)\n",
    "# module_assignment, cut_height = identify_modules(linkage_matrix, condensed_dissTOM, min_memb_cluster, deep_split)\n",
    "\n",
    "# Add the Gene name to the clustering table\n",
    "module_assignment.insert(0, 'Gene Name', list(transcriptomics_dataset_filtered))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Plot the dendrogram\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Dendogram with the modules found with dynamic tree-cutting...{ENDC}\")\n",
    "    title_figure = 'Dendogram after tree-cutting'\n",
    "\n",
    "    plt.figure(figsize=(15, 10)) \n",
    "    scipy_hierarchy.dendrogram(linkage_matrix, color_threshold=cut_height, no_labels=True)\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Genes', fontsize=10)\n",
    "    plt.ylabel('Colored following Gene Modules after dynamic tree-cutting', fontsize=10)\n",
    "    plt.axhline(y=cut_height, color='r', linestyle='--')\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")\n",
    "\n",
    "print(module_assignment['Module'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#                       TERRIBLE CODE FROM pyWGCNA          \n",
    "################################################################################################\n",
    "from scipy.cluster.hierarchy import to_tree\n",
    "from scipy.stats import rankdata\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "def get_heights(Z):\n",
    "\n",
    "    clusternode = to_tree(Z, True)\n",
    "    height = np.array([c.dist for c in clusternode[1] if c.is_leaf() is not True])\n",
    "\n",
    "    return height\n",
    "\n",
    "def get_merges(z):\n",
    "    n = z.shape[0]\n",
    "    merges = np.zeros((z.shape[0], 2), dtype=int)\n",
    "\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(2):\n",
    "            if z[i][j] <= n:\n",
    "                merges[i][j] = -(z[i][j] + 1)\n",
    "            else:\n",
    "                cluster = z[i][j] - n\n",
    "                merges[i][j] = cluster\n",
    "\n",
    "    return merges\n",
    "\n",
    "def interpolate(data, index):\n",
    "    i = round(index)\n",
    "    n = len(data)\n",
    "    if i < 1:\n",
    "        return data[1]\n",
    "    if i >= n:\n",
    "        return data[n]\n",
    "    r = index - i\n",
    "    return data[i] * (1 - r) + data[i + 1] * r\n",
    "\n",
    "def coreSizeFunc(BranchSize, minClusterSize):\n",
    "    BaseCoreSize = minClusterSize / 2 + 1\n",
    "    if BaseCoreSize < BranchSize:\n",
    "        CoreSize = int(BaseCoreSize + math.sqrt(BranchSize - BaseCoreSize))\n",
    "    else:\n",
    "        CoreSize = BranchSize\n",
    "\n",
    "    return CoreSize\n",
    "\n",
    "\n",
    "\n",
    "def cutreeHybrid(dendro, distM, cutHeight=None, minClusterSize=20, deepSplit=1,\n",
    "                     maxCoreScatter=None, minGap=None, maxAbsCoreScatter=None,\n",
    "                     minAbsGap=None, minSplitHeight=None, minAbsSplitHeight=None,\n",
    "                     externalBranchSplitFnc=None, nExternalSplits=0, minExternalSplit=None,\n",
    "                     externalSplitOptions=pd.DataFrame(), externalSplitFncNeedsDistance=None,\n",
    "                     assumeSimpleExternalSpecification=True, pamStage=True,\n",
    "                     pamRespectsDendro=True, useMedoids=False, maxPamDist=None,\n",
    "                     respectSmallClusters=True):\n",
    "    \"\"\"\n",
    "    Detect clusters in a dendorgram produced by the function hclust.\n",
    "\n",
    "    :param dendro: a hierarchical clustering dendorgram such as one returned by hclust.\n",
    "    :type dendro: ndarray\n",
    "    :param distM: Distance matrix that was used as input to hclust.\n",
    "    :type distM: pandas dataframe\n",
    "    :param cutHeight: Maximum joining heights that will be considered. It defaults to 99of the range between the 5th percentile and the maximum of the joining heights on the dendrogram.\n",
    "    :type cutHeight: int\n",
    "    :param minClusterSize: Minimum cluster size. (default = 20)\n",
    "    :type minClusterSize: int\n",
    "    :param deepSplit: Either logical or integer in the range 0 to 4. Provides a rough control over sensitivity to cluster splitting. The higher the value, the more and smaller clusters will be produced. (default = 1)\n",
    "    :type deepSplit: int or bool\n",
    "    :param maxCoreScatter: Maximum scatter of the core for a branch to be a cluster, given as the fraction of cutHeight relative to the 5th percentile of joining heights.\n",
    "    :type maxCoreScatter: int\n",
    "    :param minGap: Minimum cluster gap given as the fraction of the difference between cutHeight and the 5th percentile of joining heights.\n",
    "    :type minGap: int\n",
    "    :param maxAbsCoreScatter: Maximum scatter of the core for a branch to be a cluster given as absolute heights. If given, overrides maxCoreScatter.\n",
    "    :type maxAbsCoreScatter: int\n",
    "    :param minAbsGap: Minimum cluster gap given as absolute height difference. If given, overrides minGap.\n",
    "    :type minAbsGap: int\n",
    "    :param minSplitHeight: Minimum split height given as the fraction of the difference between cutHeight and the 5th percentile of joining heights. Branches merging below this height will automatically be merged. Defaults to zero but is used only if minAbsSplitH\n",
    "    :type minSplitHeight: int\n",
    "    :param minAbsSplitHeight: Minimum split height given as an absolute height. Branches merging below this height will automatically be merged. If not given (default), will be determined from minSplitHeight above.\n",
    "    :type minAbsSplitHeight: int\n",
    "    :param externalBranchSplitFnc: Optional function to evaluate split (dissimilarity) between two branches. Either a single function or a list in which each component is a function.\n",
    "    :param minExternalSplit: Thresholds to decide whether two branches should be merged. It should be a numeric list of the same length as the number of functions in externalBranchSplitFnc above.\n",
    "    :type minExternalSplit: list\n",
    "    :param externalSplitOptions: Further arguments to function externalBranchSplitFnc. If only one external function is specified in externalBranchSplitFnc above, externalSplitOptions can be a named list of arguments or a list with one component.\n",
    "    :type externalSplitOptions: pandas dataframe\n",
    "    :param externalSplitFncNeedsDistance: Optional specification of whether the external branch split functions need the distance matrix as one of their arguments. Either NULL or a logical list with one element per branch\n",
    "    :type externalSplitFncNeedsDistance: pandas dataframe\n",
    "    :param assumeSimpleExternalSpecification: when minExternalSplit above is a scalar (has length 1), should the function assume a simple specification of externalBranchSplitFnc and externalSplitOptions. (default = True)\n",
    "    :type assumeSimpleExternalSpecification: bool\n",
    "    :param pamStage: If TRUE, the second (PAM-like) stage will be performed. (default = True)\n",
    "    :type pamStage: bool\n",
    "    :param pamRespectsDendro: If TRUE, the PAM stage will respect the dendrogram in the sense an object can be PAM-assigned only to clusters that lie below it on the branch that the object is merged into. (default = True)\n",
    "    :type pamRespectsDendro: bool\n",
    "    :param useMedoids: if TRUE, the second stage will be use object to medoid distance; if FALSE, it will use average object to cluster distance. (default = False)\n",
    "    :param maxPamDist: Maximum object distance to closest cluster that will result in the object assigned to that cluster. Defaults to cutHeight.\n",
    "    :type maxPamDist: float\n",
    "    :param respectSmallClusters: If TRUE, branches that failed to be clusters in stage 1 only because of insufficient size will be assigned together in stage 2. If FALSE, all objects will be assigned individually. (default = False)\n",
    "    :type respectSmallClusters: bool\n",
    "\n",
    "    :return: list detailing the deteced branch structure.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    dendro_height = get_heights(dendro)\n",
    "    dendro_merge = get_merges(dendro)\n",
    "\n",
    "    chunkSize = dendro.shape[0]\n",
    "\n",
    "    if maxPamDist is None:\n",
    "        maxPamDist = cutHeight\n",
    "\n",
    "    nMerge = dendro.shape[0]\n",
    "    if nMerge < 1:\n",
    "        sys.exit(\"The given dendrogram is suspicious: number of merges is zero.\")\n",
    "    if distM is None:\n",
    "        sys.exit(\"distM must be non-NULL\")\n",
    "    if distM.shape is None:\n",
    "        sys.exit(\"distM must be a matrix.\")\n",
    "    if distM.shape[0] != nMerge + 1 or distM.shape[1] != nMerge + 1:\n",
    "        sys.exit(\"distM has incorrect dimensions.\")\n",
    "    if pamRespectsDendro and not respectSmallClusters:\n",
    "        print(\"cutreeHybrid Warning: parameters pamRespectsDendro (TRUE) \"\n",
    "                \"and respectSmallClusters (FALSE) imply contradictory intent.\\n\"\n",
    "                \"Although the code will work, please check you really intented \"\n",
    "                \"these settings for the two arguments.\", flush=True)\n",
    "\n",
    "    print(f\"{OKBLUE}Going through the merge tree...{ENDC}\")\n",
    "\n",
    "    if any(np.diag(distM) != 0):\n",
    "        np.fill_diagonal(distM, 0)\n",
    "    refQuantile = 0.05\n",
    "    refMerge = round(nMerge * refQuantile) - 1\n",
    "    if refMerge < 0:\n",
    "        refMerge = 0\n",
    "    refHeight = dendro[refMerge, 2]\n",
    "    if cutHeight is None:\n",
    "        cutHeight = 0.99 * (np.max(dendro_height) - refHeight) + refHeight\n",
    "        print(\"..cutHeight not given, setting it to\", round(cutHeight, 3),\n",
    "                \" ===>  99% of the (truncated) height range in dendro.\", flush=True)\n",
    "    else:\n",
    "        if cutHeight > np.max(dendro_height):\n",
    "            cutHeight = np.max(dendro_height)\n",
    "    if maxPamDist is None:\n",
    "        maxPamDist = cutHeight\n",
    "    nMergeBelowCut = np.count_nonzero(dendro_height <= cutHeight)\n",
    "    if nMergeBelowCut < minClusterSize:\n",
    "        print(\"cutHeight set too low: no merges below the cut.\", flush=True)\n",
    "        return pd.DataFrame({'labels': np.repeat(0, nMerge + 1, axis=0)})\n",
    "\n",
    "    if externalBranchSplitFnc is not None:\n",
    "        nExternalSplits = len(externalBranchSplitFnc)\n",
    "        if len(minExternalSplit) < 1:\n",
    "            sys.exit(\"'minExternalBranchSplit' must be given.\")\n",
    "        if assumeSimpleExternalSpecification and nExternalSplits == 1:\n",
    "            externalSplitOptions = pd.DataFrame(externalSplitOptions)\n",
    "        # todo: externalBranchSplitFnc = lapply(externalBranchSplitFnc, match.fun)\n",
    "        for es in range(nExternalSplits):\n",
    "            externalSplitOptions['tree'][es] = dendro\n",
    "            if len(externalSplitFncNeedsDistance) == 0 or externalSplitFncNeedsDistance[es]:\n",
    "                externalSplitOptions['dissimMat'][es] = distM\n",
    "\n",
    "    MxBranches = nMergeBelowCut\n",
    "    branch_isBasic = np.repeat(True, MxBranches, axis=0)\n",
    "    branch_isTopBasic = np.repeat(True, MxBranches, axis=0)\n",
    "    branch_failSize = np.repeat(False, MxBranches, axis=0)\n",
    "    branch_rootHeight = np.repeat(np.nan, MxBranches, axis=0)\n",
    "    branch_size = np.repeat(2, MxBranches, axis=0)\n",
    "    branch_nMerge = np.repeat(1, MxBranches, axis=0)\n",
    "    branch_nSingletons = np.repeat(2, MxBranches, axis=0)\n",
    "    branch_nBasicClusters = np.repeat(0, MxBranches, axis=0)\n",
    "    branch_mergedInto = np.repeat(0, MxBranches, axis=0)\n",
    "    branch_attachHeight = np.repeat(np.nan, MxBranches, axis=0)\n",
    "    branch_singletons = [np.nan] * MxBranches\n",
    "    branch_basicClusters = [np.nan] * MxBranches\n",
    "    branch_mergingHeights = [np.nan] * MxBranches\n",
    "    branch_singletonHeights = [np.nan] * MxBranches\n",
    "    nBranches = 0\n",
    "\n",
    "    defMCS = [0.64, 0.73, 0.82, 0.91, 0.95]\n",
    "    defMG = [(1.0 - defMC) * 3.0 / 4.0 for defMC in defMCS]\n",
    "    nSplitDefaults = len(defMCS)\n",
    "    if isinstance(deepSplit, bool):\n",
    "        deepSplit = pd.to_numeric(deepSplit) * (nSplitDefaults - 2)\n",
    "    if deepSplit < 0 or deepSplit > nSplitDefaults:\n",
    "        msg = \"Parameter deepSplit (value\" + str(deepSplit) + \\\n",
    "                \") out of range: allowable range is 0 through\", str(nSplitDefaults - 1)\n",
    "        sys.exit(msg)\n",
    "    if maxCoreScatter is None:\n",
    "        maxCoreScatter = interpolate(defMCS, deepSplit)\n",
    "    if minGap is None:\n",
    "        minGap = interpolate(defMG, deepSplit)\n",
    "    if maxAbsCoreScatter is None:\n",
    "        maxAbsCoreScatter = refHeight + maxCoreScatter * (cutHeight - refHeight)\n",
    "    if minAbsGap is None:\n",
    "        minAbsGap = minGap * (cutHeight - refHeight)\n",
    "    if minSplitHeight is None:\n",
    "        minSplitHeight = 0\n",
    "    if minAbsSplitHeight is None:\n",
    "        minAbsSplitHeight = refHeight + minSplitHeight * (cutHeight - refHeight)\n",
    "    nPoints = nMerge + 1\n",
    "    IndMergeToBranch = np.repeat(0, nMerge, axis=0)\n",
    "    onBranch = np.repeat(0, nPoints, axis=0)\n",
    "    RootBranch = 0\n",
    "\n",
    "    mergeDiagnostics = pd.DataFrame({'smI': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'smSize': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'smCrSc': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'smGap': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'lgI': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'lgSize': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'lgCrSc': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'lgGap': np.repeat(np.nan, nMerge, axis=0),\n",
    "                                        'merged': np.repeat(np.nan, nMerge, axis=0)})\n",
    "    if externalBranchSplitFnc is not None:\n",
    "        externalMergeDiags = pd.DataFrame(np.nan, index=list(range(nMerge)), columns=list(range(nExternalSplits)))\n",
    "\n",
    "    extender = np.repeat(0, chunkSize, axis=0)\n",
    "\n",
    "    for merge in range(nMerge):\n",
    "        if dendro_height[merge] <= cutHeight:\n",
    "            if dendro_merge[merge, 0] < 0 and dendro_merge[merge, 1] < 0:\n",
    "                nBranches = nBranches + 1\n",
    "                branch_isBasic[nBranches - 1] = True\n",
    "                branch_isTopBasic[nBranches - 1] = True\n",
    "                branch_singletons[nBranches - 1] = np.append(-1 * dendro_merge[merge, :], extender)\n",
    "                branch_basicClusters[nBranches - 1] = extender\n",
    "                branch_mergingHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2), extender)\n",
    "                branch_singletonHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2), extender)\n",
    "                IndMergeToBranch[merge] = nBranches\n",
    "                RootBranch = nBranches\n",
    "            elif np.sign(dendro_merge[merge, 0]) * np.sign(dendro_merge[merge, 1]) < 0:\n",
    "                clust = IndMergeToBranch[int(np.max(dendro_merge[merge, :])) - 1]\n",
    "\n",
    "                if clust == 0:\n",
    "                    sys.exit(\"Internal error: a previous merge has no associated cluster. Sorry!\")\n",
    "\n",
    "                gene = -1 * int(np.min(dendro_merge[merge, :]))\n",
    "                ns = branch_nSingletons[clust - 1] + 1\n",
    "                nm = branch_nMerge[clust - 1] + 1\n",
    "\n",
    "                if branch_isBasic[clust - 1]:\n",
    "                    if ns > len(branch_singletons[clust - 1]):\n",
    "                        branch_singletons[clust - 1] = np.append(branch_singletons[clust - 1], extender)\n",
    "                        branch_singletonHeights[clust - 1] = np.append(branch_singletonHeights[clust - 1], extender)\n",
    "                    branch_singletons[clust - 1][ns - 1] = gene\n",
    "                    branch_singletonHeights[clust - 1][ns - 1] = dendro_height[merge]\n",
    "                else:\n",
    "                    onBranch[int(gene) - 1] = clust\n",
    "\n",
    "                if nm >= len(branch_mergingHeights[clust - 1]):\n",
    "                    branch_mergingHeights[clust - 1] = np.append(branch_mergingHeights[clust - 1], extender)\n",
    "                branch_mergingHeights[clust - 1][nm - 1] = dendro_height[merge]\n",
    "                branch_size[clust - 1] = branch_size[clust - 1] + 1\n",
    "                branch_nMerge[clust - 1] = nm\n",
    "                branch_nSingletons[clust - 1] = ns\n",
    "                IndMergeToBranch[merge] = clust\n",
    "                RootBranch = clust\n",
    "            else:\n",
    "                clusts = IndMergeToBranch[dendro_merge[merge, :].astype(int) - 1]\n",
    "                sizes = branch_size[clusts - 1]\n",
    "                rnk = rankdata(sizes, method=\"ordinal\")\n",
    "                small = clusts[rnk[0] - 1]\n",
    "                large = clusts[rnk[1] - 1]\n",
    "                sizes = sizes[rnk - 1]\n",
    "\n",
    "                if branch_isBasic[small - 1]:\n",
    "                    coresize = coreSizeFunc(branch_nSingletons[small - 1], minClusterSize)\n",
    "                    Core = np.array(branch_singletons[small - 1][np.arange(coresize)] - 1, dtype=int)\n",
    "                    SmAveDist = np.mean(distM.iloc[Core, Core].sum() / coresize - 1)\n",
    "                else:\n",
    "                    SmAveDist = 0\n",
    "\n",
    "                if branch_isBasic[large - 1]:\n",
    "                    coresize = coreSizeFunc(branch_nSingletons[large - 1], minClusterSize)\n",
    "                    Core = np.array(branch_singletons[large - 1][np.arange(int(coresize))] - 1, dtype=int)\n",
    "                    LgAveDist = np.mean(distM.iloc[Core, Core].sum() / coresize - 1)\n",
    "                else:\n",
    "                    LgAveDist = 0\n",
    "\n",
    "                mergeDiagnostics.loc[merge, :] = [small,\n",
    "                                                    branch_size[small - 1],\n",
    "                                                    SmAveDist,\n",
    "                                                    dendro_height[merge] - SmAveDist,\n",
    "                                                    large,\n",
    "                                                    branch_size[large - 1],\n",
    "                                                    LgAveDist,\n",
    "                                                    dendro_height[merge] - LgAveDist,\n",
    "                                                    None]\n",
    "                SmallerScores = [branch_isBasic[small - 1],\n",
    "                                    branch_size[small - 1] < minClusterSize,\n",
    "                                    SmAveDist > maxAbsCoreScatter,\n",
    "                                    dendro_height[merge] - SmAveDist < minAbsGap,\n",
    "                                    dendro_height[merge] < minAbsSplitHeight]\n",
    "                if SmallerScores[0] * np.count_nonzero(SmallerScores[1:]) > 0:\n",
    "                    DoMerge = True\n",
    "                    SmallerFailSize = ~np.logical_or(SmallerScores[2], SmallerScores[3])\n",
    "                else:\n",
    "                    LargerScores = [branch_isBasic[large - 1],\n",
    "                                    branch_size[large - 1] < minClusterSize,\n",
    "                                    LgAveDist > maxAbsCoreScatter,\n",
    "                                    dendro_height[merge] - LgAveDist < minAbsGap,\n",
    "                                    dendro_height[merge] < minAbsSplitHeight]\n",
    "                    if LargerScores[0] * np.count_nonzero(LargerScores[1:]) > 0:\n",
    "                        DoMerge = True\n",
    "                        SmallerFailSize = ~np.logical_or(LargerScores[2], LargerScores[3])\n",
    "                        x = small\n",
    "                        small = large\n",
    "                        large = x\n",
    "                        sizes = np.flip(sizes)\n",
    "                    else:\n",
    "                        DoMerge = False\n",
    "\n",
    "                if DoMerge:\n",
    "                    mergeDiagnostics['merged'][merge] = 1\n",
    "\n",
    "                if not DoMerge and nExternalSplits > 0 and branch_isBasic[small - 1] and branch_isBasic[large - 1]:\n",
    "                    branch1 = branch_singletons[large - 1][np.arange(sizes[1])]\n",
    "                    branch2 = branch_singletons[small - 1][np.arange(sizes[0])]\n",
    "                    es = 0\n",
    "                    while es < nExternalSplits and not DoMerge:\n",
    "                        es = es + 1\n",
    "                        args = pd.DataFrame({'externalSplitOptions': externalSplitOptions[es - 1],\n",
    "                                                'branch1': branch1, 'branch2': branch2})\n",
    "                        # TODO: extSplit = do.call(externalBranchSplitFnc[[es]], args)\n",
    "                        extSplit = None\n",
    "                        DoMerge = extSplit < minExternalSplit[es - 1]\n",
    "                        externalMergeDiags[merge, es - 1] = extSplit\n",
    "                        mergeDiagnostics['merged'][merge] = 0\n",
    "                        if DoMerge:\n",
    "                            mergeDiagnostics['merged'][merge] = 2\n",
    "\n",
    "                if DoMerge:\n",
    "                    branch_failSize[small - 1] = SmallerFailSize\n",
    "                    branch_mergedInto[small - 1] = large\n",
    "                    branch_attachHeight[small - 1] = dendro_height[merge]\n",
    "                    branch_isTopBasic[small - 1] = False\n",
    "                    nss = branch_nSingletons[small - 1]\n",
    "                    nsl = branch_nSingletons[large - 1]\n",
    "                    ns = nss + nsl\n",
    "\n",
    "                    if branch_isBasic[large - 1]:\n",
    "                        nExt = np.ceil((ns - len(branch_singletons[large - 1])) / chunkSize)\n",
    "                        if nExt > 0:\n",
    "                            branch_singletons[large - 1] = np.append(branch_singletons[large - 1],\n",
    "                                                                        np.repeat(extender, nExt))\n",
    "                            branch_singletonHeights[large - 1] = np.append(branch_singletonHeights[large - 1],\n",
    "                                                                            np.repeat(extender, nExt))\n",
    "\n",
    "                        branch_singletons[large - 1][np.arange(nsl, ns)] = branch_singletons[small - 1][\n",
    "                            np.arange(nss)]\n",
    "                        branch_singletonHeights[large - 1][np.arange(nsl, ns)] = branch_singletonHeights[small - 1][\n",
    "                            np.arange(nss)]\n",
    "                        branch_nSingletons[large] = ns\n",
    "                    else:\n",
    "                        if not branch_isBasic[small - 1]:\n",
    "                            sys.exit(\"Internal error: merging two composite clusters. Sorry!\")\n",
    "\n",
    "                        onBranch[branch_singletons[small - 1][branch_singletons[small - 1] != 0] - 1] = large\n",
    "\n",
    "                    nm = branch_nMerge[large - 1] + 1\n",
    "\n",
    "                    if nm > len(branch_mergingHeights[large - 1]):\n",
    "                        branch_mergingHeights[large - 1] = np.append(branch_mergingHeights[large - 1], extender)\n",
    "\n",
    "                    branch_mergingHeights[large - 1][nm - 1] = dendro_height[merge]\n",
    "                    branch_nMerge[large - 1] = nm\n",
    "                    branch_size[large - 1] = branch_size[small - 1] + branch_size[large - 1]\n",
    "                    IndMergeToBranch[merge] = large\n",
    "                    RootBranch = large\n",
    "                else:\n",
    "                    if branch_isBasic[large - 1] and not branch_isBasic[small - 1]:\n",
    "                        x = large\n",
    "                        large = small\n",
    "                        small = x\n",
    "                        sizes = np.flip(sizes)\n",
    "\n",
    "                    if branch_isBasic[large - 1] or (pamStage and pamRespectsDendro):\n",
    "                        nBranches = nBranches + 1\n",
    "                        branch_attachHeight[[large - 1, small - 1]] = dendro_height[merge]\n",
    "                        branch_mergedInto[[large - 1, small - 1]] = nBranches\n",
    "                        if branch_isBasic[small - 1]:\n",
    "                            addBasicClusters = small\n",
    "                        else:\n",
    "                            addBasicClusters = branch_basicClusters[small - 1]\n",
    "                        if branch_isBasic[large - 1]:\n",
    "                            addBasicClusters = np.append(addBasicClusters, large)\n",
    "                        else:\n",
    "                            addBasicClusters = np.append(addBasicClusters, branch_basicClusters[large - 1])\n",
    "                        branch_isBasic[nBranches - 1] = False\n",
    "                        branch_isTopBasic[nBranches - 1] = False\n",
    "                        branch_basicClusters[nBranches - 1] = addBasicClusters\n",
    "                        branch_mergingHeights[nBranches - 1] = np.append(np.repeat(dendro_height[merge], 2),\n",
    "                                                                            extender)\n",
    "                        branch_nMerge[nBranches - 1] = 2\n",
    "                        branch_size[nBranches - 1] = np.sum(sizes)\n",
    "                        branch_nBasicClusters[nBranches - 1] = len(addBasicClusters)\n",
    "                        IndMergeToBranch[merge] = nBranches\n",
    "                        RootBranch = nBranches\n",
    "                    else:\n",
    "                        addBasicClusters = [small] if branch_isBasic[small - 1] else branch_basicClusters[small - 1]\n",
    "\n",
    "                        nbl = branch_nBasicClusters[large - 1]\n",
    "                        nb = branch_nBasicClusters[large - 1] + len(addBasicClusters)\n",
    "\n",
    "                        if nb > len(branch_basicClusters[large - 1]):\n",
    "                            nExt = np.ceil((nb - len(branch_basicClusters[large - 1])) / chunkSize)\n",
    "                            branch_basicClusters[large - 1] = np.append(branch_basicClusters[large - 1],\n",
    "                                                                        np.repeat(extender, nExt))\n",
    "\n",
    "                        branch_basicClusters[large - 1][np.arange(nbl, nb)] = addBasicClusters\n",
    "                        branch_nBasicClusters[large - 1] = nb\n",
    "                        branch_size[large - 1] = branch_size[large - 1] + branch_size[small - 1]\n",
    "                        nm = branch_nMerge[large - 1] + 1\n",
    "\n",
    "                        if nm > len(branch_mergingHeights[large - 1]):\n",
    "                            branch_mergingHeights[large - 1] = np.append(branch_mergingHeights[large - 1], extender)\n",
    "\n",
    "                        branch_mergingHeights[large - 1][nm - 1] = dendro_height[merge]\n",
    "                        branch_nMerge[large - 1] = nm\n",
    "                        branch_attachHeight[small - 1] = dendro_height[merge]\n",
    "                        branch_mergedInto[small - 1] = large\n",
    "                        IndMergeToBranch[merge] = large\n",
    "                        RootBranch = large\n",
    "\n",
    "    isCluster = np.repeat(False, nBranches)\n",
    "    SmallLabels = np.repeat(0, nPoints)\n",
    "\n",
    "    for clust in range(nBranches):\n",
    "        if np.isnan(branch_attachHeight[clust]):\n",
    "            branch_attachHeight[clust] = cutHeight\n",
    "        if branch_isTopBasic[clust]:\n",
    "            coresize = coreSizeFunc(branch_nSingletons[clust], minClusterSize)\n",
    "            Core = np.array(branch_singletons[clust][np.arange(coresize)] - 1, dtype=int)\n",
    "            CoreScatter = np.mean(distM.iloc[Core, Core].sum() / (coresize - 1))\n",
    "            isCluster[clust] = (branch_isTopBasic[clust] and branch_size[clust] >= minClusterSize and\n",
    "                                CoreScatter < maxAbsCoreScatter and branch_attachHeight[\n",
    "                                    clust] - CoreScatter > minAbsGap)\n",
    "        else:\n",
    "            CoreScatter = 0\n",
    "        if branch_failSize[clust]:\n",
    "            SmallLabels[branch_singletons[clust].astype(int) - 1] = clust + 1\n",
    "\n",
    "    if not respectSmallClusters:\n",
    "        SmallLabels = np.repeat(0, nPoints)\n",
    "\n",
    "    Colors = np.zeros((nPoints,))\n",
    "    coreLabels = np.zeros((nPoints,))\n",
    "    clusterBranches = np.where(isCluster)[0].tolist()\n",
    "    branchLabels = np.zeros((nBranches,))\n",
    "    color = 0\n",
    "\n",
    "    for clust in clusterBranches:\n",
    "        color = color + 1\n",
    "        Colors[branch_singletons[clust][branch_singletons[clust] != 0] - 1] = color\n",
    "        SmallLabels[branch_singletons[clust][branch_singletons[clust] != 0] - 1] = 0\n",
    "        coresize = coreSizeFunc(branch_nSingletons[clust], minClusterSize)\n",
    "        Core = np.array(branch_singletons[clust][np.arange(coresize)] - 1, dtype=int)\n",
    "        coreLabels[Core] = color\n",
    "        branchLabels[clust] = color\n",
    "\n",
    "    Labeled = np.where(Colors != 0)[0].tolist()\n",
    "    Unlabeled = np.where(Colors == 0)[0].tolist()\n",
    "    nUnlabeled = len(Unlabeled)\n",
    "    UnlabeledExist = nUnlabeled > 0\n",
    "\n",
    "    if len(Labeled) > 0:\n",
    "        LabelFac = pd.Categorical(Colors[Labeled])\n",
    "        nProperLabels = len(LabelFac.categories)\n",
    "    else:\n",
    "        nProperLabels = 0\n",
    "\n",
    "    if pamStage and UnlabeledExist and nProperLabels > 0:\n",
    "        nPAMed = 0\n",
    "        if useMedoids:\n",
    "            Medoids = np.repeat(0, nProperLabels)\n",
    "            ClusterRadii = np.repeat(0, nProperLabels)\n",
    "            for cluster in range(1, nProperLabels + 1):\n",
    "                InCluster = np.arange(1, nPoints + 1)[Colors == cluster]\n",
    "                DistInCluster = distM.iloc[InCluster - 1, InCluster - 1]\n",
    "                DistSums = DistInCluster.sum(axis=0)\n",
    "                Medoids[cluster - 1] = InCluster[DistSums.idxmin()]\n",
    "                ClusterRadii[cluster - 1] = np.max(DistInCluster[:, DistSums.idxmin()])\n",
    "\n",
    "            if respectSmallClusters:\n",
    "                FSmallLabels = pd.Categorical(SmallLabels)\n",
    "                SmallLabLevs = pd.to_numeric(FSmallLabels.categories)\n",
    "                nSmallClusters = len(FSmallLabels.categories) - (SmallLabLevs[1] == 0)\n",
    "\n",
    "                if nSmallClusters > 0:\n",
    "                    for sclust in SmallLabLevs[SmallLabLevs != 0]:\n",
    "                        InCluster = np.where(SmallLabels == sclust)[0].tolist()\n",
    "                        if pamRespectsDendro:\n",
    "                            onBr = np.unique(onBranch[InCluster])\n",
    "                            if len(onBr) > 1:\n",
    "                                msg = \"Internal error: objects in a small cluster are marked to belong\\n \" \\\n",
    "                                        \"to several large branches:\" + str(onBr)\n",
    "                                sys.exit(msg)\n",
    "\n",
    "                            if onBr > 0:\n",
    "                                basicOnBranch = branch_basicClusters[onBr[0] - 1]\n",
    "                                labelsOnBranch = branchLabels[basicOnBranch - 1]\n",
    "                            else:\n",
    "                                labelsOnBranch = None\n",
    "                        else:\n",
    "                            labelsOnBranch = list(range(1, nProperLabels + 1))\n",
    "\n",
    "                        DistInCluster = distM.iloc[InCluster, InCluster]\n",
    "\n",
    "                        if len(labelsOnBranch) > 0:\n",
    "                            if len(InCluster) > 1:\n",
    "                                DistSums = DistInCluster.sum(axis=1)\n",
    "                                smed = InCluster[DistSums.idxmin()]\n",
    "                                DistToMeds = distM.iloc[Medoids[labelsOnBranch - 1] - 1, smed]\n",
    "                                closest = DistToMeds.idxmin()\n",
    "                                DistToClosest = DistToMeds[closest]\n",
    "                                closestLabel = labelsOnBranch[closest]\n",
    "                                if DistToClosest < ClusterRadii[closestLabel - 1] or DistToClosest < maxPamDist:\n",
    "                                    Colors[InCluster] = closestLabel\n",
    "                                    nPAMed = nPAMed + len(InCluster)\n",
    "                            else:\n",
    "                                Colors[InCluster] = -1\n",
    "                        else:\n",
    "                            Colors[InCluster] = -1\n",
    "\n",
    "            Unlabeled = np.where(Colors == 0)[0].tolist()\n",
    "            if len(Unlabeled > 0):\n",
    "                for obj in Unlabeled:\n",
    "                    if pamRespectsDendro:\n",
    "                        onBr = onBranch[obj]\n",
    "                        if onBr > 0:\n",
    "                            basicOnBranch = branch_basicClusters[onBr - 1]\n",
    "                            labelsOnBranch = branchLabels[basicOnBranch - 1]\n",
    "                        else:\n",
    "                            labelsOnBranch = None\n",
    "                    else:\n",
    "                        labelsOnBranch = np.arange(nProperLabels)\n",
    "\n",
    "                    if labelsOnBranch is not None:\n",
    "                        UnassdToMedoidDist = distM.iloc[Medoids[labelsOnBranch - 1] - 1, obj]\n",
    "                        nearest = UnassdToMedoidDist.idxmin()\n",
    "                        NearestCenterDist = UnassdToMedoidDist[nearest]\n",
    "                        nearestMed = labelsOnBranch[nearest]\n",
    "                        if NearestCenterDist < ClusterRadii[nearestMed - 1] or NearestCenterDist < maxPamDist:\n",
    "                            Colors[obj] = nearestMed\n",
    "                            nPAMed = nPAMed + 1\n",
    "                UnlabeledExist = (sum(Colors == 0) > 0)\n",
    "        else:\n",
    "            ClusterDiam = np.zeros((nProperLabels,))\n",
    "            for cluster in range(nProperLabels):\n",
    "                InCluster = np.where(Colors == cluster)[0].tolist()\n",
    "                nInCluster = len(InCluster)\n",
    "                DistInCluster = distM.iloc[InCluster, InCluster]\n",
    "                if nInCluster > 1:\n",
    "                    AveDistInClust = DistInCluster.sum(axis=1) / (nInCluster - 1)\n",
    "                    AveDistInClust.reset_index(drop=True, inplace=True)\n",
    "                    ClusterDiam[cluster] = AveDistInClust.max()\n",
    "                else:\n",
    "                    ClusterDiam[cluster] = 0\n",
    "\n",
    "            ColorsX = Colors.copy()\n",
    "            if respectSmallClusters:\n",
    "                FSmallLabels = pd.Categorical(SmallLabels)\n",
    "                SmallLabLevs = pd.to_numeric(FSmallLabels.categories)\n",
    "                nSmallClusters = len(FSmallLabels.categories) - (SmallLabLevs[0] == 0)\n",
    "                if nSmallClusters > 0:\n",
    "                    if pamRespectsDendro:\n",
    "                        for sclust in SmallLabLevs[SmallLabLevs != 0]:\n",
    "                            InCluster = list(range(nPoints))[SmallLabels == sclust]\n",
    "                            onBr = pd.unique(onBranch[InCluster])\n",
    "                            if len(onBr) > 1:\n",
    "                                msg = \"Internal error: objects in a small cluster are marked to belong\\n\" \\\n",
    "                                        \"to several large branches:\" + str(onBr)\n",
    "                                sys.exit(msg)\n",
    "                            if onBr > 0:\n",
    "                                basicOnBranch = branch_basicClusters[onBr[0] - 1]\n",
    "                                labelsOnBranch = branchLabels[basicOnBranch - 1]\n",
    "                                useObjects = ColorsX in np.unique(labelsOnBranch)\n",
    "                                DistSClustClust = distM.iloc[InCluster, useObjects]\n",
    "                                MeanDist = DistSClustClust.mean(axis=0)\n",
    "                                useColorsFac = ColorsX[useObjects]  # pd.Categorical(ColorsX[useObjects])\n",
    "                                MeanDist = pd.DataFrame({'MeanDist': MeanDist, 'useColorsFac': useColorsFac})\n",
    "                                MeanMeanDist = MeanDist.groupby(\n",
    "                                    'useColorsFac').mean()  # tapply(MeanDist, useColorsFac, mean)\n",
    "                                nearest = MeanMeanDist[['MeanDist']].idxmin().astype(int) - 1\n",
    "                                NearestDist = MeanMeanDist[['MeanDist']].min()\n",
    "                                if np.logical_or(np.all(NearestDist < ClusterDiam[nearest]),\n",
    "                                                    NearestDist < maxPamDist).tolist()[0]:\n",
    "                                    Colors[InCluster] = nearest\n",
    "                                    nPAMed = nPAMed + len(InCluster)\n",
    "                                else:\n",
    "                                    Colors[InCluster] = -1\n",
    "                    else:\n",
    "                        labelsOnBranch = list(range(nProperLabels))\n",
    "                        useObjects = np.where(ColorsX != 0)[0].tolist()\n",
    "                        for sclust in SmallLabLevs[SmallLabLevs != 0]:\n",
    "                            InCluster = np.where(SmallLabels == sclust)[0].tolist()\n",
    "                            DistSClustClust = distM.iloc[InCluster, useObjects]\n",
    "                            MeanDist = DistSClustClust.mean(axis=0)\n",
    "                            useColorsFac = ColorsX[useObjects]  # pd.Categorical(ColorsX[useObjects])\n",
    "                            MeanDist = pd.DataFrame({'MeanDist': MeanDist, 'useColorsFac': useColorsFac})\n",
    "                            MeanMeanDist = MeanDist.groupby(\n",
    "                                'useColorsFac').mean()  # tapply(MeanDist, useColorsFac, mean)\n",
    "                            nearest = MeanMeanDist[['MeanDist']].idxmin().astype(int) - 1\n",
    "                            NearestDist = MeanMeanDist[['MeanDist']].min()\n",
    "                            if np.logical_or(np.all(NearestDist < ClusterDiam[nearest]),\n",
    "                                                NearestDist < maxPamDist).tolist()[0]:\n",
    "                                Colors[InCluster] = nearest\n",
    "                                nPAMed = nPAMed + len(InCluster)\n",
    "                            else:\n",
    "                                Colors[InCluster] = -1\n",
    "\n",
    "            Unlabeled = np.where(Colors == 0)[0].tolist()\n",
    "            if len(Unlabeled) > 0:\n",
    "                if pamRespectsDendro:\n",
    "                    unlabOnBranch = Unlabeled[onBranch[Unlabeled] > 0]\n",
    "                    for obj in unlabOnBranch:\n",
    "                        onBr = onBranch[obj]\n",
    "                        basicOnBranch = branch_basicClusters[onBr - 1]\n",
    "                        labelsOnBranch = branchLabels[basicOnBranch]\n",
    "                        useObjects = ColorsX in np.unique(labelsOnBranch)\n",
    "                        useColorsFac = ColorsX[useObjects]  # pd.Categorical(ColorsX[useObjects])\n",
    "                        UnassdToClustDist = distM.iloc[useObjects, obj].groupby(\n",
    "                            'useColorsFac').mean()  # tapply(distM[useObjects, obj], useColorsFac, mean)\n",
    "                        nearest = UnassdToClustDist.idxmin().astype(int) - 1\n",
    "                        NearestClusterDist = UnassdToClustDist[nearest]\n",
    "                        nearestLabel = pd.to_numeric(useColorsFac.categories[nearest])\n",
    "                        if np.logical_or(np.all(NearestClusterDist < ClusterDiam[nearest]),\n",
    "                                            NearestClusterDist < maxPamDist).tolist()[0]:\n",
    "                            Colors[obj] = nearest\n",
    "                            nPAMed = nPAMed + 1\n",
    "                else:\n",
    "                    useObjects = np.where(ColorsX != 0)[0].tolist()\n",
    "                    useColorsFac = ColorsX[useObjects]  # pd.Categorical(ColorsX[useObjects])\n",
    "                    tmp = pd.DataFrame(distM.iloc[useObjects, Unlabeled])\n",
    "                    tmp['group'] = useColorsFac\n",
    "                    UnassdToClustDist = tmp.groupby(\n",
    "                        ['group']).mean()  # apply(distM[useObjects, Unlabeled], 2, tapply, useColorsFac, mean)\n",
    "                    nearest = np.subtract(UnassdToClustDist.idxmin(axis=0),\n",
    "                                            np.ones(UnassdToClustDist.shape[1])).astype(\n",
    "                        int)  # apply(UnassdToClustDist, 2, which.min)\n",
    "                    nearestDist = UnassdToClustDist.min(axis=0)  # apply(UnassdToClustDist, 2, min)\n",
    "                    nearestLabel = nearest + 1\n",
    "                    sumAssign = np.sum(np.logical_or(nearestDist < ClusterDiam[nearest], nearestDist < maxPamDist))\n",
    "                    assign = np.where(np.logical_or(nearestDist < ClusterDiam[nearest], nearestDist < maxPamDist))[\n",
    "                        0].tolist()\n",
    "                    tmp = [Unlabeled[x] for x in assign]\n",
    "                    Colors[tmp] = [nearestLabel.iloc[x] for x in assign]\n",
    "                    nPAMed = nPAMed + sumAssign\n",
    "\n",
    "    Colors[np.where(Colors < 0)[0].tolist()] = 0\n",
    "    UnlabeledExist = (np.count_nonzero(Colors == 0) > 0)\n",
    "    NumLabs = list(map(int, Colors.copy()))\n",
    "    Sizes = pd.DataFrame(NumLabs).value_counts().sort_index()\n",
    "    OrdNumLabs = pd.DataFrame({\"Module\": NumLabs, \"Rank\": np.repeat(1, len(NumLabs))})\n",
    "\n",
    "    if UnlabeledExist:\n",
    "        if len(Sizes) > 1:\n",
    "            SizeRank = np.insert(stats.rankdata(-1 * Sizes[1:len(Sizes)], method='ordinal') + 1, 0, 1)\n",
    "        else:\n",
    "            SizeRank = [1]\n",
    "        for i in range(len(NumLabs)):\n",
    "            OrdNumLabs.loc[i, 'Rank'] = SizeRank[NumLabs[i] - 1]\n",
    "    else:\n",
    "        SizeRank = stats.rankdata(-1 * Sizes, method='ordinal')\n",
    "        for i in range(len(NumLabs)):\n",
    "            OrdNumLabs.loc[i, 'Rank'] = SizeRank[NumLabs[i] - 1]\n",
    "\n",
    "    OrdNumLabs.insert(0, 'Gene', range(1, len(OrdNumLabs['Module']) +1, 1))\n",
    "\n",
    "    print(\"\\tDone..\\n\")\n",
    "\n",
    "    OrdNumLabs.Rank = OrdNumLabs.Rank - UnlabeledExist\n",
    "    return OrdNumLabs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Temporary turning back to pandas, but should be reimplemented with numpy\n",
    "dissTOM = pd.DataFrame(dissTOM_np, columns=transcriptomics_dataset_filtered.columns, index=transcriptomics_dataset_filtered.columns)\n",
    "\n",
    "\n",
    "\n",
    "## Call to the copied function from pyWGCNA\n",
    "module_assignment = cutreeHybrid(dendro=linkage_matrix, distM=dissTOM, deepSplit=2, pamRespectsDendro=False,\n",
    "                                         minClusterSize=20)\n",
    "print(module_assignment['Module'].value_counts())\n",
    "\n",
    "# Add the Gene name to the clustering table\n",
    "module_assignment.insert(0, 'Gene Name', list(transcriptomics_dataset_filtered))\n",
    "module_assignment = module_assignment.drop('Rank', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 7: Calculate EigenGenes for all identified Modules\n",
    "print(f\"{BOLD}{OKBLUE}Calculating EigenGenes...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "####################################### FUNCTIONS #######################################\n",
    "def expression_profile_for_cluster(module_assignment, transcriptomics_dataset_filtered):\n",
    "    expression_profile = transcriptomics_dataset_filtered.T\n",
    "    expression_profile = expression_profile.reset_index()\n",
    "    merged_df = pd.merge(module_assignment, expression_profile, left_on='Gene Name', right_on='index', how='inner')\n",
    "    merged_df = merged_df.drop('index', axis=1)\n",
    "    merged_df = merged_df.drop('Gene', axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "def calculate_eigen_genes(expression_profiles):\n",
    "    eigengenes = []\n",
    "    \n",
    "    ## Iterate through each module to calculate its eigen gene\n",
    "    for module in expression_profiles['Module'].unique():\n",
    "\n",
    "        # Skip module 0 as it represents unassigned genes\n",
    "        if module == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Extract the Expression Profile for all genes in this module\n",
    "            module_expression_profile = expression_profiles[expression_profiles['Module'] == module].iloc[:, 2:]  # Exclude Gene Name and Module columns\n",
    "\n",
    "            # Perform PCA on the expression data of the current module\n",
    "            pca = PCA(n_components=1)\n",
    "            pca.fit(module_expression_profile)\n",
    "            \n",
    "            # The first principal component is the eigengene\n",
    "            eigengene = pca.components_[0]\n",
    "\n",
    "            # Create a DataFrame for the eigengene with the correct sample labels and the module id\n",
    "            eigengene_df = pd.DataFrame(eigengene.reshape(1, -1), columns=expression_profiles.columns[2:])\n",
    "            eigengene_df.insert(0, 'Module', module)\n",
    "            eigengenes.append(eigengene_df)\n",
    "    \n",
    "    \n",
    "    return pd.concat(eigengenes, ignore_index=True)\n",
    "\n",
    "\n",
    "## Prepare the dataframe so that it contains all the info we need to continue\n",
    "expression_profiles = expression_profile_for_cluster(module_assignment, transcriptomics_dataset_filtered)\n",
    "\n",
    "## Call the function to calculate the eigengenes\n",
    "eigen_genes = calculate_eigen_genes(expression_profiles)\n",
    "print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "## Plot the Expression Profile for the Eigengenes across pacients\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Eigengene Expression Profile Across Samples...{ENDC}\")\n",
    "    title_figure = 'Eigengene Expression Profile Across Samples'\n",
    "\n",
    "    sample_labels = eigen_genes.columns[1:]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for index, row in eigen_genes.iterrows():\n",
    "        # Convert eigengene array stored as list back to numpy array for plotting\n",
    "        eigengene_values = np.array(row[1:].values)\n",
    "        \n",
    "        # Plotting the eigengene values\n",
    "        plt.plot(sample_labels, eigengene_values, label=f'Module {row[\"Module\"]}')\n",
    "\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Samples (pacients)', fontsize=10)\n",
    "    plt.ylabel('Eigengene Expression Level', fontsize=10)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yscale('log') # ONLY if there is big outliers that make reading data dificult\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir + title_figure, dpi=dpi_general)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 8: Module-trait relationship\n",
    "print(f\"{BOLD}{OKBLUE}Calculating Module-trait relationship...{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "####################################### FUNCTIONS #######################################\n",
    "def encode_categorical(df, column):\n",
    "    \"\"\"\n",
    "    Encode categorical variables.\n",
    "    \"\"\"\n",
    "    if df[column].dtype == 'object':\n",
    "        if column == 'Age':  # Assuming Age has an order\n",
    "            df[column] = df[column].str.extract('(\\d+)').astype(int)\n",
    "        else:  # One-hot encode other categorical variables\n",
    "            dummies = pd.get_dummies(df[column], prefix=column)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_correlations(eigen_genes, trait_dataset, trait_columns):\n",
    "    \"\"\"\n",
    "    Calculate correlations between eigengenes and traits.\n",
    "    \"\"\"\n",
    "    correlations = pd.DataFrame()\n",
    "    p_values = pd.DataFrame()\n",
    "\n",
    "    for trait in trait_columns:\n",
    "        # Get encoded traits to be able to calculate Spearman Correlation\n",
    "        trait_dataset = encode_categorical(trait_dataset, trait)\n",
    "        trait_data = trait_dataset.filter(like=trait)  \n",
    "\n",
    "        for module in eigen_genes['Module'].unique():\n",
    "            # Get the Expression Profile vector for each EigenGene (representing each module). \n",
    "            module_data = eigen_genes[eigen_genes['Module'] == module].drop('Module', axis=1).T\n",
    "            module_data.columns = ['eigengene']\n",
    "\n",
    "            # Calculate correlations for all samples and all eigengenes\n",
    "            for trait_name in trait_data.columns:\n",
    "                cor, p_val = stats.spearmanr(module_data['eigengene'], trait_data[trait_name])\n",
    "                correlations.loc[module, trait_name] = cor\n",
    "                p_values.loc[module, trait_name] = p_val\n",
    "\n",
    "    return correlations, p_values\n",
    "\n",
    "\n",
    "# Run the Analysis, encoding the variables as categorical, and calculating correlation and p-value\n",
    "trait_columns = list(trait_dataset.columns[1:] )\n",
    "correlations, p_values = calculate_correlations(eigen_genes, trait_dataset, trait_columns)\n",
    "print(f\"{BOLD}{OKBLUE}Done\\n\\n{ENDC}\")\n",
    "\n",
    "\n",
    "\n",
    "## Plot the dendrogram\n",
    "if PLOTS_WANTED:\n",
    "    print(f\"{BOLD}{OKBLUE}Plotting and Saving the Module EigenGene to Clinical Trait Correlation...{ENDC}\")\n",
    "    title_figure = 'Module Eigengene to Clinical Trait Correlation'\n",
    "\n",
    "    annotations = correlations.round(3).astype(str) + '\\n(' + p_values.round(5).astype(str) + ')'\n",
    "\n",
    "    plt.figure(figsize=(40, 40)) \n",
    "    sns.heatmap(correlations, annot=annotations.values, fmt='', cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "    plt.title(title_figure, fontsize=20)\n",
    "    plt.xlabel('Selected Clincal Traits', fontsize=10)\n",
    "    plt.ylabel('Identified Modules, represented by their EigenGene', fontsize=10)\n",
    "    plt.savefig(figures_dir + title_figure, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"{BOLD}{OKBLUE}Done{ENDC}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WGCNA_test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
