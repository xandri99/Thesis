{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification with Neural Networks and Graph Neural Networks\n",
    "\n",
    "Based on the Keras guide in https://keras.io/examples/graph/gnn_citations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import re\n",
    "from matplotlib_venn import venn3\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move out of the notebook folder to access datasets\n",
    "working_dir = os.getcwd()\n",
    "working_dir = working_dir.strip('notebooks')\n",
    "data_dir = working_dir + 'data/PROTECTED_DATA/BGI_Expression_Data/'\n",
    "\n",
    "\n",
    "# Transcriptomics Data \n",
    "transcriptomics_TumorOnly_dir = data_dir + 'CRC.SW.mRNA.symbol.TPM_TumorOnly.csv'\n",
    "transcriptomics_dataset = pd.read_csv(transcriptomics_TumorOnly_dir, index_col=0)\n",
    "\n",
    "# Classification Tags\n",
    "labels_classification_dir = data_dir + 'PrimarySiteDisease_for_TumorSamples_Classification.csv' # Using only tumor samples\n",
    "labels = pd.read_csv(labels_classification_dir, index_col=0)\n",
    "\n",
    "\n",
    "# Figures Saving output dir\n",
    "\n",
    "\n",
    "# Convert The directory to the name of the column\n",
    "trait_used_as_label = labels_classification_dir.replace(data_dir, '').replace('_for_TumorSamples_Classification.csv', '')\n",
    "trait_used_as_label = re.sub(r'(?<=\\w)([A-Z])', r' \\1', trait_used_as_label) # Add spaces before the capital letters for formatting\n",
    "\n",
    "# Convert labels to categorical values\n",
    "#class_values = labels[trait_used_as_label].astype('category').cat.codes\n",
    "#labels['label'] = class_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore distribution and balance of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = labels[trait_used_as_label].value_counts()\n",
    "\n",
    "# Plotting a histogram of class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title(f'Number of Elements for Each Class in {trait_used_as_label}')\n",
    "plt.xlabel(trait_used_as_label)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dummified Dataframe for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the Labels column (with the selected clinical trail) with the transcriptomics dataset\n",
    "transcriptomics_labeled_df = pd.merge(left=transcriptomics_dataset, left_index=True, right=labels, right_index=True, how='inner')\n",
    "\n",
    "\n",
    "## We convert the Samples ids and the Labels into zero-based indices.\n",
    "class_values = labels[trait_used_as_label].unique()\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "pacient_idx = {name: idx for idx, name in enumerate(transcriptomics_dataset.index.unique())}\n",
    "\n",
    "\n",
    "## We update the dataframe with this dummified values\n",
    "transcriptomics_labeled_df = transcriptomics_labeled_df.rename(index=pacient_idx)\n",
    "transcriptomics_labeled_df[trait_used_as_label] = transcriptomics_labeled_df[trait_used_as_label].apply(lambda name: class_idx[name])\n",
    "transcriptomics_labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into stratified Train, Test and Validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, validation_data = [], [], []\n",
    "\n",
    "# Create train and test splits\n",
    "for _, group_data in transcriptomics_labeled_df.groupby(trait_used_as_label):\n",
    "    # Declare the % of train, test, validate\n",
    "    train_split = 0.7\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # Build the subsets for each category\n",
    "    random_selection = np.random.rand(len(group_data.index))\n",
    "    train_subset = random_selection <= train_split\n",
    "    test_subset = (random_selection > train_split) & (random_selection <= (train_split + test_split))\n",
    "    val_subset = random_selection > (train_split + test_split)\n",
    "\n",
    "    # Reformat the data\n",
    "    train_data.append(group_data[train_subset])\n",
    "    test_data.append(group_data[test_subset])\n",
    "    validation_data.append(group_data[val_subset])\n",
    "\n",
    "# Concatenate and shuffle the subsets from each group\n",
    "train_data = pd.concat(train_data).sample(frac=1)\n",
    "test_data = pd.concat(test_data).sample(frac=1)\n",
    "validation_data = pd.concat(validation_data).sample(frac=1)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "print(\"Validation data shape:\", validation_data.shape)\n",
    "\n",
    "\n",
    "# Plot Venn diagram to visualize overlap\n",
    "venn3([set(train_data.index), set(test_data.index), set(validation_data.index)], ('Train', 'Test', 'Validation'))\n",
    "plt.title(\"Overlap between Train, Test, and Validation Sets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the genes's names, removing the label column\n",
    "feature_names = list(set(transcriptomics_labeled_df.columns) - {trait_used_as_label})\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(class_idx)\n",
    "\n",
    "## Create train,test and validation features as a numpy array.\n",
    "x_train = train_data[feature_names].to_numpy()\n",
    "x_test = test_data[feature_names].to_numpy()\n",
    "x_validation = validation_data[feature_names].to_numpy()\n",
    "\n",
    "## Create train and test targets as a numpy array.\n",
    "y_train = train_data[trait_used_as_label]\n",
    "y_test = test_data[trait_used_as_label]\n",
    "y_validation = validation_data[trait_used_as_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Train and Evaluate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter configuration - Standard values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "num_epochs = 300\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions compiles and trains an input model using the given training data, displays the loss and accuracy curves, and Implement Feedforward Network (FFN) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, x_train, y_train, x_test, y_test):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    # Create an early stopping callback.\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    # Fit the model.\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "def display_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history.history[\"loss\"])\n",
    "    ax1.plot(history.history[\"val_loss\"])\n",
    "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(history.history[\"acc\"])\n",
    "    ax2.plot(history.history[\"val_acc\"])\n",
    "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):\n",
    "    inputs = layers.Input(shape=(num_features,), name=\"input_features\")\n",
    "    x = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block1\")(inputs)\n",
    "    for block_idx in range(4):\n",
    "        # Create an FFN block.\n",
    "        x1 = create_ffn(hidden_units, dropout_rate, name=f\"ffn_block{block_idx + 2}\")(x)\n",
    "        # Add skip connection.\n",
    "        x = layers.Add(name=f\"skip_connection{block_idx + 2}\")([x, x1])\n",
    "    # Compute logits.\n",
    "    logits = layers.Dense(num_classes, name=\"logits\")(x)\n",
    "    # Create the model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits, name=\"baseline\")\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(hidden_units, num_classes, dropout_rate)\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "history = run_training(baseline_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "display_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the baseline model on the VALIDATION data split. \n",
    "_, test_accuracy = baseline_model.evaluate(x=x_validation, y=y_validation, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Graph Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Graph from the data, to feed it to the GNN\n",
    "\n",
    "For this example, we will use the simplest approach to this, that it consists on building a simple correlation matrix between the pacients, and not the genes.\n",
    "This approach shall be improved in further version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> SIMPLE VERSION : using the whole correlation matrix  -> 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this architecutre, each node is a pacient, and the node features is the full transcriptome of that pacient [num_nodes, num_features]\n",
    "node_features = tf.cast(transcriptomics_labeled_df[feature_names].to_numpy(), dtype=tf.dtypes.float32) # We keep the label out!!\n",
    "\n",
    "\n",
    "\n",
    "## Fully linked network, and the weight the strength of the correlation\n",
    "# Turn dataframe into NumPy matrix for efficiency\n",
    "transcriptomics_np = transcriptomics_labeled_df[feature_names].to_numpy()\n",
    "\n",
    "# Calculate Correlation matrix (correlation between pacients)\n",
    "pacients_correlation_matrix, _ = stats.spearmanr(transcriptomics_np, axis=1)\n",
    "\n",
    "# Normalize edge weights and adjust self-connections\n",
    "pacients_correlation_matrix = (pacients_correlation_matrix + 1) / 2\n",
    "np.fill_diagonal(pacients_correlation_matrix, 0)\n",
    "pacients_correlation_matrix = np.clip(pacients_correlation_matrix, 0, 1)\n",
    "\n",
    "# Flatten the correlation matrix (only upper half) (excluding the diagonal (k=1))\n",
    "rows, cols = np.triu_indices(n=pacients_correlation_matrix.shape[0], k=1)\n",
    "flat_edge_weights = pacients_correlation_matrix[rows, cols]\n",
    "edge_weights = tf.constant(flat_edge_weights, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "## Create the edges array\n",
    "edges = np.vstack((rows, cols))\n",
    "\n",
    "\n",
    "# Create graph info tuple with node_features, edges, and edge_weights.\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "\n",
    "# Display shapes of node_features and edge_weights\n",
    "print(\"Node Features shape:\", node_features.shape)\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Edge Weights shape:\", edge_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> COMPLEX VERSION : using Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this architecutre, each node is a pacient, and the node features is the full transcriptome of that pacient [num_nodes, num_features]\n",
    "node_features = tf.cast(transcriptomics_labeled_df[feature_names].to_numpy(), dtype=tf.dtypes.float32) # We keep the label out!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Fully linked network, and the weight the strength of the Adjacency\n",
    "# Turn dataframe into NumPy matrix for efficiency\n",
    "transcriptomics_np = transcriptomics_labeled_df[feature_names].to_numpy()\n",
    "\n",
    "# Calculate Correlation matrix (correlation between pacients)\n",
    "pacients_correlation_matrix, _ = stats.spearmanr(transcriptomics_np, axis=1)\n",
    "\n",
    "# Construct the Adjacency Matrix, Set correlations under 0.9 to 0, maintain others, and ensure ranges\n",
    "adjacency_matrix_np = np.where(pacients_correlation_matrix > 0.95, pacients_correlation_matrix, 0)\n",
    "np.fill_diagonal(adjacency_matrix_np, 0)\n",
    "adjacency_matrix_np = np.clip(adjacency_matrix_np, 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Flatten the Adjacency matrix (only upper half) (excluding the diagonal (k=1))\n",
    "rows, cols = np.triu_indices(n=adjacency_matrix_np.shape[0], k=1)\n",
    "flat_edge_weights = adjacency_matrix_np[rows, cols]\n",
    "\n",
    "# Filter to keep only non-zero weights\n",
    "non_zero_indices = np.where(flat_edge_weights > 0)[0]\n",
    "\n",
    "# Use these indices to create the edges array with only non-zero connections\n",
    "edges = np.vstack((rows[non_zero_indices], cols[non_zero_indices]))\n",
    "\n",
    "# Convert the filtered flat_edge_weights to a TensorFlow constant, if needed\n",
    "edge_weights = tf.constant(flat_edge_weights[non_zero_indices], dtype=tf.float32)\n",
    "\n",
    "# With the fully connected graph and Adjacency Matrix binarized for corr>0.9, we get --> 81%\n",
    "# Graph only keeping links over 0.9, represented by the correlation value, we get --> 84.51%\n",
    "\n",
    "print(f\"Number of links removed: {len(flat_edge_weights) - len(non_zero_indices)}, \\\n",
    "      so {100*(len(flat_edge_weights) - len(non_zero_indices))/len(flat_edge_weights)}%\\n\")\n",
    "\n",
    "\n",
    "# Create graph info tuple with node_features, edges, and edge_weights.\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "\n",
    "# Display shapes of node_features and edge_weights\n",
    "print(\"Node Features shape:\", node_features.shape)\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Edge Weights shape:\", edge_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a graph convolution layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gru(hidden_units, dropout_rate):\n",
    "    inputs = keras.layers.Input(shape=(2, hidden_units[0]))\n",
    "    x = inputs\n",
    "    for units in hidden_units:\n",
    "      x = layers.GRU(\n",
    "          units=units,\n",
    "          activation=\"tanh\",\n",
    "          recurrent_activation=\"sigmoid\",\n",
    "          return_sequences=True,\n",
    "          dropout=dropout_rate,\n",
    "          return_state=False,\n",
    "          recurrent_dropout=dropout_rate,\n",
    "      )(x)\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "class GraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"concat\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        if self.combination_type == \"gru\":\n",
    "            self.update_fn = create_gru(hidden_units, dropout_rate)\n",
    "        else:\n",
    "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        if weights is not None:\n",
    "            messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "        if self.aggregation_type == \"sum\":\n",
    "            aggregated_message = tf.math.unsorted_segment_sum(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"mean\":\n",
    "            aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        elif self.aggregation_type == \"max\":\n",
    "            aggregated_message = tf.math.unsorted_segment_max(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        if self.combination_type == \"gru\":\n",
    "            # Create a sequence of two elements for the GRU layer.\n",
    "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"concat\":\n",
    "            # Concatenate the node_repesentations and aggregated_messages.\n",
    "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        elif self.combination_type == \"add\":\n",
    "            # Add node_repesentations and aggregated_messages.\n",
    "            h = node_repesentations + aggregated_messages\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
    "\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Process the inputs to produce the node_embeddings.\n",
    "\n",
    "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
    "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
    "        \"\"\"\n",
    "\n",
    "        node_repesentations, edges, edge_weights = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "\n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(\n",
    "            node_indices, neighbour_messages, node_repesentations\n",
    "        )\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a graph neural network node classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNNNodeClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        num_classes,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        # Set edge_weights to ones if not provided.\n",
    "        if self.edge_weights is None:\n",
    "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
    "        # Scale edge_weights to sum to 1.\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2 = GraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "    def call(self, input_node_indices):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(self.node_features)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x1 + x\n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        # Compute logits\n",
    "        return self.compute_logits(node_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and summary of the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = GNNNodeClassifier(\n",
    "    graph_info=graph_info,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnn_model\",\n",
    ")\n",
    "\n",
    "print(\"GNN output shape:\", gnn_model([1, 10, 100]))\n",
    "\n",
    "gnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Display Learning Curves for the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "x_train = train_data.index.to_numpy()\n",
    "x_test = test_data.index.to_numpy()\n",
    "\n",
    "history = run_training(gnn_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "display_learning_curves(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = validation_data.index.to_numpy()\n",
    "\n",
    "_, test_accuracy = gnn_model.evaluate(x=x_validation, y=y_validation, verbose=0)\n",
    "print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
