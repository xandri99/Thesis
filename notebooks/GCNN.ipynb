{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Graph and network analysis\n",
    "import networkx as nx\n",
    "import graph_tool.all as gt\n",
    "\n",
    "from scipy import stats\n",
    "# from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Machine Learning and Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.utils import from_networkx\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Parallel \n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Others\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move out of the notebook folder to access datasets\n",
    "working_dir = os.getcwd()\n",
    "working_dir = working_dir.strip('notebooks')\n",
    "data_dir = working_dir + 'data/PROTECTED_DATA/BGI_Expression_Data/'\n",
    "\n",
    "\n",
    "## Load the dataset\n",
    "# Transcriptomics Data \n",
    "transcriptomics_TumorOnly_dir = data_dir + 'CRC.SW.mRNA.symbol.TPM_TumorOnly.csv'\n",
    "transcriptomics_dataset = pd.read_csv(transcriptomics_TumorOnly_dir, index_col=0)\n",
    "\n",
    "# Classification Tags\n",
    "labels_classification_dir = data_dir + 'TumourStage_for_TumorSamples_Classification.csv' # Using only tumor samples\n",
    "labels = pd.read_csv(labels_classification_dir, index_col=0)\n",
    "\n",
    "\n",
    "# Figures Saving output dir\n",
    "\n",
    "\n",
    "# Convert The directory to the name of the column\n",
    "trait_used_as_label = labels_classification_dir.replace(data_dir, '').replace('_for_TumorSamples_Classification.csv', '')\n",
    "trait_used_as_label = re.sub(r'(?<=\\w)([A-Z])', r' \\1', trait_used_as_label) # Add spaces before the capital letters for formatting\n",
    "\n",
    "# Convert labels to categorical values\n",
    "class_values = labels[trait_used_as_label].astype('category').cat.codes\n",
    "labels['label'] = class_values\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Make a subset to save RAM\n",
    "subset_dataset_size = 1000\n",
    "transcriptomics_dataset = transcriptomics_dataset.iloc[:, :subset_dataset_size] \n",
    "\n",
    "# RAM usage estimation in GB\n",
    "RAM_estimate = (subset_dataset_size * subset_dataset_size * 8) / (1024**3)\n",
    "print(f\"The aproximated RAM to analyse this size of dataset is: {RAM_estimate} GB\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_encoding(labels):\n",
    "    \"\"\"Check if labels are integer-encoded.\"\"\"\n",
    "    if not pd.api.types.is_integer_dtype(labels):\n",
    "        print(\"Labels are not integer-encoded. Encoding now...\")\n",
    "    else:\n",
    "        print(\"Labels are properly integer-encoded.\")\n",
    "\n",
    "def check_dataset_balance(labels):\n",
    "    \"\"\"Check for imbalances in the label distribution.\"\"\"\n",
    "    class_counts = labels.value_counts()\n",
    "    print(\"Class distribution:\\n\", class_counts)\n",
    "    max_count = class_counts.max()\n",
    "    min_count = class_counts.min()\n",
    "    if max_count / min_count > 2:  # Threshold for imbalance might need adjustment\n",
    "        print(\"Dataset is imbalanced.\")\n",
    "    else:\n",
    "        print(\"Dataset is relatively balanced.\")\n",
    "\n",
    "\n",
    "\n",
    "check_label_encoding(labels['label'])\n",
    "print('\\n')\n",
    "check_dataset_balance(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for the Model\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Preprocess the data using the same method as in the WGCNA approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_th = 1\n",
    "\n",
    "\n",
    "# Filter out genes with low expression across all samples\n",
    "transcriptomics_clean = transcriptomics_dataset.loc[:, (transcriptomics_dataset > expression_th).any(axis=0)].copy()\n",
    "\n",
    "# Apply log2 transformation to all values except for the first column (gene identifiers)\n",
    "transcriptomics_clean.iloc[:, 1:] = np.log2(transcriptomics_clean.iloc[:, 1:] + 1)\n",
    "\n",
    "# Print the number of genes removed\n",
    "num_genes_removed = transcriptomics_dataset.shape[1] - transcriptomics_clean.shape[1]\n",
    "print(f\"preprocess_TPM_outlier_deletion function removed {num_genes_removed} genes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency Matrix\n",
    "\n",
    "We can build different types of adjacency matrix, that, as seen in the NN and GNN, have a great effect in the efectivness of the model.\n",
    "This is the approach taken as per the paper:\n",
    "\n",
    "To create the co-expression graph, Spearman correlation was calculated to generate a correlation matrix between each gene in the dataset.\n",
    "Spearman Correlation is a widely adopted method to assess monotonic linear or non-linear relationships in sequencing data. \n",
    "If the correlation between two genes is >0.6 with a p < 0.05, a weight of 1 is placed in an adjacency matrix, otherwise 0. If there is no correlation >0.6 with a given gene, then that gene is removed from the gene list, leading to the total of genes in the co-expression graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spearman Correlation and p-values\n",
    "transcriptomics_np = transcriptomics_clean.to_numpy()\n",
    "correlations, pvalues = stats.spearmanr(transcriptomics_np)\n",
    "\n",
    "# Construct the Adjacency Matrix\n",
    "adjacency_matrix_np = (correlations > 0.6) & (pvalues < 0.05)\n",
    "adjacency_matrix_np = adjacency_matrix_np.astype(int)\n",
    "\n",
    "# Remove Isolated Genes - does not correlate with any other gene\n",
    "is_not_isolated = adjacency_matrix_np.sum(axis=1) > 0\n",
    "filtered_adjacency_matrix_np = adjacency_matrix_np[is_not_isolated, :][:, is_not_isolated]\n",
    "np.fill_diagonal(filtered_adjacency_matrix_np, 0)\n",
    "\n",
    "# Same data with different formatting\n",
    "adjacency_matrix = pd.DataFrame(adjacency_matrix_np, index=transcriptomics_clean.columns, columns=transcriptomics_clean.columns)\n",
    "filtered_adjacency_matrix = adjacency_matrix.loc[is_not_isolated, is_not_isolated]\n",
    "\n",
    "\n",
    "print(f'{transcriptomics_clean.shape[1]-filtered_adjacency_matrix_np.shape[0]} genes were removed as Isolated Genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap\n",
    "plt.imshow(filtered_adjacency_matrix_np, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Binary Adjacency Matrix Heatmap')\n",
    "plt.xlabel('Genes')\n",
    "plt.ylabel('Genes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph representation of the Adjacency Matrix\n",
    "\n",
    "We Tranform the Adjacency Matrix into a graph object. \n",
    "\n",
    "Adjacency Matrix is turned into an unweighted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the adjacency matrix\n",
    "adjG = nx.from_numpy_array(filtered_adjacency_matrix_np)\n",
    "\n",
    "# Add expression data )expression of a gene across all samples) as node features\n",
    "filtered_genes = transcriptomics_clean.columns[is_not_isolated]\n",
    "for i, gene in enumerate(filtered_genes):\n",
    "    adjG.nodes[i]['expression'] = transcriptomics_clean.loc[:, gene].values\n",
    "\n",
    "# Convert the networkx graph to a PyTorch Geometric graph\n",
    "adjG_py = from_networkx(adjG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph information from Adjacency Matrix\")\n",
    "print(\"Number of nodes:\", adjG.number_of_nodes())\n",
    "print(\"Number of edges:\", adjG.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node attributes\n",
    "print(\"Node attributes:\")\n",
    "for node, data in adjG.nodes(data=True):\n",
    "    print(f\"Node {node}: {data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph representation of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Parallelization to speed up the process, and different data structure (graph in C++)\n",
    "\n",
    "def build_graph_tool_graph(filtered_adjacency_matrix_np, node_features):\n",
    "    g = gt.Graph(directed=False)\n",
    "    g.add_edge_list(np.transpose(filtered_adjacency_matrix_np.nonzero()))\n",
    "    \n",
    "    # Add a vertex property for expression\n",
    "    vprop_expression = g.new_vp(\"double\")\n",
    "    g.vertex_properties[\"expression\"] = vprop_expression\n",
    "\n",
    "    # Set the expression data\n",
    "    for v in g.vertices():\n",
    "        vprop_expression[v] = node_features[int(v)]\n",
    "\n",
    "    return g\n",
    "\n",
    "def process_sample(sample_id, transcriptomics_clean, filtered_genes, template_graph, labels):\n",
    "    # Node features and conversion to tensor\n",
    "    node_features = transcriptomics_clean.loc[sample_id, filtered_genes].values.astype(np.float32)\n",
    "    \n",
    "    # Create a copy of the graph structure with new expression values\n",
    "    sampleG = build_graph_tool_graph(template_graph, node_features)\n",
    "    \n",
    "    # Data object preparation (to mimic PyTorch Geometric structure if necessary)\n",
    "    sample_data = {\n",
    "        'graph': sampleG,\n",
    "        'features': torch.tensor(node_features, dtype=torch.float).view(-1, 1),\n",
    "        'label': torch.tensor([labels.loc[sample_id, 'label']], dtype=torch.long)\n",
    "    }\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "\n",
    "sampleG = build_graph_tool_graph(filtered_adjacency_matrix_np, np.zeros(filtered_adjacency_matrix_np.shape[0]))\n",
    "\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(transcriptomics_clean.index)\n",
    "\n",
    "# List to hold results\n",
    "samplesGpy_list = []\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "# Create a ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks\n",
    "    future_to_sample = {executor.submit(process_sample, sample_id, transcriptomics_clean, filtered_genes, sampleG, labels): sample_id for sample_id in transcriptomics_clean.index}\n",
    "    \n",
    "    # Process as they complete\n",
    "    for i, future in enumerate(as_completed(future_to_sample), 1):\n",
    "        sample_data = future.result()\n",
    "        samplesGpy_list.append(sample_data)\n",
    "        \n",
    "        # Progress output\n",
    "        progress_message = f\"Processed {i}/{total_samples} samples ({(i / total_samples) * 100:.2f}% complete)\"\n",
    "        sys.stdout.write(\"\\r\" + progress_message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Parallelization to speed up the process (runs 4 times faster)\n",
    "\n",
    "def process_sample(sample_id, transcriptomics_clean, filtered_genes, sampleG, labels):\n",
    "    # Node features and conversion to tensor\n",
    "    node_features = transcriptomics_clean.loc[sample_id, filtered_genes]\n",
    "    node_features_tensor = torch.tensor(node_features.values, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Update graph with current sample's expression data\n",
    "    tempG = sampleG.copy()\n",
    "    for node in tempG.nodes:\n",
    "        tempG.nodes[node]['expression'] = node_features_tensor[node].item()\n",
    "    \n",
    "    # Convert to PyTorch Geometric Data\n",
    "    sampleG_py = from_networkx(tempG)\n",
    "    sampleG_py.x = node_features_tensor\n",
    "    sampleG_py.y = torch.tensor([labels.loc[sample_id, 'label']], dtype=torch.long)\n",
    "    \n",
    "    return sampleG_py\n",
    "\n",
    "sampleG = nx.from_numpy_array(filtered_adjacency_matrix_np)\n",
    "nx.set_edge_attributes(sampleG, values=1, name='weight')\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(transcriptomics_clean.index)\n",
    "\n",
    "# List to hold results\n",
    "samplesGpy_list = []\n",
    "\n",
    "# Create a ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks\n",
    "    future_to_sample = {executor.submit(process_sample, sample_id, transcriptomics_clean, filtered_genes, sampleG, labels): sample_id for sample_id in transcriptomics_clean.index}\n",
    "    \n",
    "    # Process as they complete\n",
    "    for i, future in enumerate(as_completed(future_to_sample), 1):\n",
    "        sampleG_py = future.result()\n",
    "        samplesGpy_list.append(sampleG_py)\n",
    "        \n",
    "        # Progress output\n",
    "        progress_message = f\"Processed {i}/{total_samples} samples ({(i / total_samples) * 100:.2f}% complete)\"\n",
    "        sys.stdout.write(\"\\r\" + progress_message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working in batches, still not great\n",
    "\n",
    "# Create a constant graph structure based on gene correlations, that will represent each sample\n",
    "sampleG = nx.from_numpy_array(filtered_adjacency_matrix_np)\n",
    "nx.set_edge_attributes(sampleG, values=1, name='weight')\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 32\n",
    "total_samples = len(transcriptomics_clean.index)\n",
    "num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "\n",
    "samplesGpy_list = []\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = min((batch_index + 1) * batch_size, total_samples)\n",
    "\n",
    "    # Initialize the graph template for this batch\n",
    "    batch_sampleG = sampleG.copy()\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        sample_id = transcriptomics_clean.index[i]\n",
    "        node_features = transcriptomics_clean.loc[sample_id, filtered_genes]\n",
    "        node_features_tensor = torch.tensor(node_features.values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        # Update node features\n",
    "        for node in batch_sampleG.nodes:\n",
    "            batch_sampleG.nodes[node]['expression'] = node_features_tensor[node].item()\n",
    "\n",
    "        # Convert to PyG Data\n",
    "        sampleG_py = from_networkx(batch_sampleG)\n",
    "        sampleG_py.x = node_features_tensor\n",
    "\n",
    "        # Set labels\n",
    "        label = labels.loc[sample_id, 'label']\n",
    "        sampleG_py.y = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "        samplesGpy_list.append(sampleG_py)\n",
    "\n",
    "    # Print progress\n",
    "    progress_message = f\"Processed {end_index}/{total_samples} samples ({(end_index / total_samples) * 100:.2f}% complete)\"\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 10 hours to run - very ineficient\n",
    "\n",
    "# Create a constant graph structure based on gene correlations, that will represent each sample\n",
    "sampleG = nx.from_numpy_array(filtered_adjacency_matrix_np)\n",
    "nx.set_edge_attributes(sampleG, values=1, name='weight')\n",
    "\n",
    "# Prepare a list to hold the graph data objects\n",
    "samplesGpy_list = []\n",
    "\n",
    "# Initialize counters for total samples and processed samples\n",
    "total_samples = len(transcriptomics_clean.index)\n",
    "processed_samples = 0\n",
    "\n",
    "for sample_id in transcriptomics_clean.index:\n",
    "    # The features of each node is the full expression profile of the sample for the 'active' genes, as a tensor\n",
    "    node_features = transcriptomics_clean.loc[sample_id, filtered_genes]\n",
    "    # Turn features into a Tensor with only the expression values\n",
    "    node_features_tensor = torch.tensor(node_features.values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "    # Turn this sample into graph using the template-graph\n",
    "    for node in sampleG.nodes:\n",
    "        sampleG.nodes[node]['expression'] = node_features_tensor[node].item()\n",
    "\n",
    "    # Convert NetworkX graph to PyTorch Geometric Data\n",
    "    sampleG_py = from_networkx(sampleG)\n",
    "    sampleG_py.x = node_features_tensor\n",
    "\n",
    "    # Add the label for each sample\n",
    "    label = labels.loc[sample_id, 'label']\n",
    "    sampleG_py.y = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "    # Append the sample object to the list\n",
    "    samplesGpy_list.append(sampleG_py)\n",
    "\n",
    "    # Print Updatable counter\n",
    "    processed_samples += 1\n",
    "    percentage_processed = (processed_samples / total_samples) * 100\n",
    "    progress_message = f\"Processed {processed_samples}/{total_samples} samples ({percentage_processed:.2f}% complete)\"\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Graph Convolutional Network (GCN) or Graph Isomorphism Network (GIN) or Graph Attention Networks (GATs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and preparation for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Splitting data indices for training and validation\n",
    "train_test, val_set = train_test_split(samplesGpy_list, test_size=0.05, random_state=42)\n",
    "train_set, test_set = train_test_split(train_test, test_size=0.2, random_state=42)  # 20% of the remaining for test\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Calculating weights for the imbalancement adjustment\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels['label']), \n",
    "    y=labels['label'].to_numpy()\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch\n",
    "for batch in train_loader:\n",
    "    data = batch  # Assuming batch contains a Data object\n",
    "    break\n",
    "\n",
    "# Print details about the batch\n",
    "print(\"Features (x):\", data.x.shape)\n",
    "print(\"Edge index (edge_index):\", data.edge_index.shape)\n",
    "print(\"Batch index (batch):\", data.batch.shape)  # Shows to which graph each node belongs\n",
    "print(\"Labels (y):\", data.y)\n",
    "\n",
    "# Example of plotting - visualize the distribution of labels in this batch\n",
    "labeled_labels = data.y.numpy()  # Convert labels to numpy array if not already\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(labeled_labels, bins=np.arange(labeled_labels.min(), labeled_labels.max()+2) - 0.5, edgecolor='black')\n",
    "plt.title('Distribution of labeled_labels in a Batch')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(np.unique(labeled_labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_classes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        # First Graph Convolution Layer\n",
    "        self.conv1 = GCNConv(num_features, num_hidden)\n",
    "        self.bn1 = BatchNorm(num_hidden)  # Batch normalization layer\n",
    "\n",
    "        # Second Graph Convolution Layer\n",
    "        self.conv2 = GCNConv(num_hidden, num_hidden // 2)\n",
    "        self.bn2 = BatchNorm(num_hidden // 2)  # Batch normalization layer\n",
    "\n",
    "        # Third Graph Convolution Layer (newly added)\n",
    "        self.conv3 = GCNConv(num_hidden // 2, num_hidden // 2)\n",
    "        self.bn3 = BatchNorm(num_hidden // 2)  # Batch normalization layer\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(num_hidden // 2, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Third GCN layer (newly added)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)  # Apply batch normalization\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Pooling and final classification\n",
    "        x = global_mean_pool(x, batch)  # Aggregate features to graph level\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model and loss function with class weights\n",
    "num_features = 1  # Since each node has 1 feature: expression value\n",
    "num_hidden = 256   # Number of features in hidden layer\n",
    "num_classes = labels['label'].nunique()  # Number of unique labels\n",
    "\n",
    "model = GCNModel(num_features=num_features, num_hidden=num_hidden, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)  # Move data to the appropriate device\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return average_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)  # Move data to the appropriate device\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return average_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics storage for execution\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Cuda for the lab\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# Scheduler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Time: {epoch_time:.2f}s.      \\\n",
    "          Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_accuracies, label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in val_loader:\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        y_true.extend(data.y.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Get predictions and true labels\n",
    "y_true, y_pred = validate()\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
